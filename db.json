{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/BlueLake/source/favicon.png","path":"favicon.png","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/fancybox/jquery.fancybox.min.css","path":"fancybox/jquery.fancybox.min.css","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/fancybox/jquery.fancybox.min.js","path":"fancybox/jquery.fancybox.min.js","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/js/jquery-3.4.1.min.js","path":"js/jquery-3.4.1.min.js","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/js/search.json.js","path":"js/search.json.js","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/iconfont/iconfont.eot","path":"css/iconfont/iconfont.eot","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/iconfont/iconfont.svg","path":"css/iconfont/iconfont.svg","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/iconfont/iconfont.ttf","path":"css/iconfont/iconfont.ttf","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/iconfont/iconfont.woff","path":"css/iconfont/iconfont.woff","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/iconfont/iconfont.woff2","path":"css/iconfont/iconfont.woff2","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/images/alipay-pay.jpg","path":"css/images/alipay-pay.jpg","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/images/alipay.jpg","path":"css/images/alipay.jpg","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/images/banner-dark.jpg","path":"css/images/banner-dark.jpg","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/images/wechat-pay.jpg","path":"css/images/wechat-pay.jpg","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/images/wechat.jpg","path":"css/images/wechat.jpg","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/localshare/css/share.styl","path":"localshare/css/share.styl","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/localshare/fonts/iconfont.eot","path":"localshare/fonts/iconfont.eot","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/localshare/fonts/iconfont.svg","path":"localshare/fonts/iconfont.svg","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/localshare/fonts/iconfont.ttf","path":"localshare/fonts/iconfont.ttf","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/localshare/fonts/iconfont.woff","path":"localshare/fonts/iconfont.woff","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/localshare/js/qrcode.js","path":"localshare/js/qrcode.js","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/localshare/js/social-share.js","path":"localshare/js/social-share.js","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/images/11banner.jpg","path":"css/images/11banner.jpg","modified":0,"renderable":1},{"_id":"themes/BlueLake/source/css/images/11banner-dark.jpg","path":"css/images/11banner-dark.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1613240803660},{"_id":"themes/BlueLake/LICENSE","hash":"3e191ca3f51efc111863c4941051291a696ef43e","modified":1613241038706},{"_id":"themes/BlueLake/_config.yml","hash":"08f11f59866e1dd4dac5174f63b232f92e6f5389","modified":1637999135938},{"_id":"themes/BlueLake/.gitignore","hash":"999db0c9fe03f35cae9145141fe3931239fd34df","modified":1613241038706},{"_id":"themes/BlueLake/README.md","hash":"2633c3f19548bcd7f1da23ab7b3eb6cd9f58a1bb","modified":1613241038706},{"_id":"themes/BlueLake/package.json","hash":"36a90f881b63e6806ab36762dd85a5f56927eab3","modified":1613241038715},{"_id":"themes/BlueLake/languages/de-DE.yml","hash":"ec0dd4b7badb6c0f687444a6c7e3646d29b4cc74","modified":1613241038707},{"_id":"themes/BlueLake/languages/en.yml","hash":"97e5709af8d432b53331e25acdb0aeef0636c8d4","modified":1613241038707},{"_id":"themes/BlueLake/languages/es-ES.yml","hash":"d99da24ae1a943ad573f831134e15dfc0dacce42","modified":1613241038707},{"_id":"themes/BlueLake/languages/ko.yml","hash":"764246632c02a70943fcc80362df2eb76adc8a21","modified":1613241038707},{"_id":"themes/BlueLake/languages/zh-CN.yml","hash":"510a05266ae304523e73b3bdb0ebc62fea064195","modified":1613241038708},{"_id":"themes/BlueLake/languages/fr-FR.yml","hash":"b05fa1f82fb9f6dc01c2fab612410b109077c359","modified":1613241038707},{"_id":"themes/BlueLake/languages/tr.yml","hash":"d3474815ebee20942a620b59fd33b24a9079715b","modified":1613241038708},{"_id":"themes/BlueLake/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1613241038714},{"_id":"themes/BlueLake/languages/zh-TW.yml","hash":"71cfea9fe6165f3e2cd1505935dd82e8e0bd0348","modified":1613241038708},{"_id":"themes/BlueLake/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1613241038714},{"_id":"themes/BlueLake/layout/index.ejs","hash":"a357447e69d38afa2263052751bc3f1f01f7ca5d","modified":1613241038714},{"_id":"themes/BlueLake/layout/page.ejs","hash":"cc86a83b9edfa0ec21a1d6a1c99e0f772a064ed9","modified":1613241038715},{"_id":"themes/BlueLake/layout/layout.ejs","hash":"5a48460d7fd5b76b837622fda72cc37769f7c199","modified":1613241038714},{"_id":"themes/BlueLake/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1613241038715},{"_id":"themes/BlueLake/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1613241038715},{"_id":"themes/BlueLake/source/favicon.png","hash":"98f0ecbdcdc1a0e8e52f4d786cbc011e5e06fa65","modified":1613241038725},{"_id":"themes/BlueLake/layout/_partial/about.ejs","hash":"ee79a6ef1cc405efefd504c5363045af3c5ec7a1","modified":1637919884042},{"_id":"themes/BlueLake/layout/_partial/archive.ejs","hash":"40dc9f65d07ff4b0e05ce58b1fb25b7d4a2dfd06","modified":1613241038709},{"_id":"themes/BlueLake/layout/_partial/after-footer.ejs","hash":"14d3ff8b7b5315b2ad83df171272faa00930a3f4","modified":1613241038709},{"_id":"themes/BlueLake/layout/_partial/baidu-analytics.ejs","hash":"0bbb6af521596dcbfb2b61191bef5fe196587996","modified":1613241038709},{"_id":"themes/BlueLake/layout/_partial/archive-post.ejs","hash":"61e74e87a7fb3ad1af7d5ebcb38d80318e8a8621","modified":1613241038709},{"_id":"themes/BlueLake/layout/_partial/article.ejs","hash":"3ca513b7d67c8cb4357d7250cb21abbff84afc41","modified":1613241038709},{"_id":"themes/BlueLake/layout/_partial/commonts-script.ejs","hash":"6ef6932547fca85611d46edc9545b501356f5af6","modified":1613241038710},{"_id":"themes/BlueLake/layout/_partial/google-analytics.ejs","hash":"2ea7442ea1e1a8ab4e41e26c563f58413b59a3d0","modified":1613241038710},{"_id":"themes/BlueLake/layout/_partial/footer.ejs","hash":"36aea06813c022235967fa7227d8b9875e3b973c","modified":1613241038710},{"_id":"themes/BlueLake/layout/_partial/head.ejs","hash":"1a68cc55fab575c009cf46ecaa21b32831de9979","modified":1613241038710},{"_id":"themes/BlueLake/layout/_partial/gauges-analytics.ejs","hash":"21a1e2a3907d1a3dad1cd0ab855fe6735f233c74","modified":1613241038710},{"_id":"themes/BlueLake/layout/_partial/pagination.ejs","hash":"da420eaf6b3672b446c8584a84d9b06bb4e2da90","modified":1613241038710},{"_id":"themes/BlueLake/layout/_partial/header.ejs","hash":"36690b8c2b7a041770037e699aa530bc6d439d33","modified":1613241038710},{"_id":"themes/BlueLake/layout/_widget/category.ejs","hash":"48636efbaf95e90d7246f5b996be8919a43c0f19","modified":1613241038713},{"_id":"themes/BlueLake/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1613241038713},{"_id":"themes/BlueLake/layout/_widget/links.ejs","hash":"8f0f9530c9e1ff4b19945ae8cb93328ca92f4cf9","modified":1613241038713},{"_id":"themes/BlueLake/layout/_widget/archive.ejs","hash":"389b54e076c85b7cf102c0952a538a193964fa37","modified":1613241038713},{"_id":"themes/BlueLake/layout/_widget/recent_posts.ejs","hash":"af6e1f3de744a229cee30137d7a36b92e998c178","modified":1613241038714},{"_id":"themes/BlueLake/layout/_widget/tag.ejs","hash":"5c11d583f3dec803dfdfe75ee64ac424a838d94e","modified":1613241038714},{"_id":"themes/BlueLake/source/css/_extend.styl","hash":"9691b1ef1faba9b7bd034fd7b34baf9670aad62a","modified":1613241038715},{"_id":"themes/BlueLake/layout/_widget/tagcloud.ejs","hash":"2df7a22f3513c66977e63d8d6e03f94e4f8b9f72","modified":1613241038714},{"_id":"themes/BlueLake/source/css/_variables.styl","hash":"1066460a9f23ab81c6a3ffc6927729933f68ea97","modified":1637912640858},{"_id":"themes/BlueLake/source/css/style.styl","hash":"92eef2a05e9beb87a7ec2e47ea47fe1f62407ca2","modified":1613241038723},{"_id":"themes/BlueLake/source/fancybox/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1613241038724},{"_id":"themes/BlueLake/source/js/script.js","hash":"b9441363ff8fe829d90295ea947960338a1a020a","modified":1613241038726},{"_id":"themes/BlueLake/source/js/search.json.js","hash":"ec601a6db4c68a7d9c2d440951c6e26ebfbc1f3a","modified":1613241038726},{"_id":"themes/BlueLake/layout/_partial/post/comments-count.ejs","hash":"b41c771d033d7a732f65940727b9bee6f99f7040","modified":1613241038711},{"_id":"themes/BlueLake/layout/_partial/post/category.ejs","hash":"9357b1e9ec68bd4a4513bb803d4336fce196edda","modified":1613241038711},{"_id":"themes/BlueLake/layout/_partial/post/comments.ejs","hash":"98de4a73ff6da2dc1179a1966a3725f535321291","modified":1637919917871},{"_id":"themes/BlueLake/layout/_partial/post/date.ejs","hash":"da28f2ee8bb2e5fe6179ce4a5da204d198625bd9","modified":1613241038711},{"_id":"themes/BlueLake/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1613241038711},{"_id":"themes/BlueLake/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1613241038712},{"_id":"themes/BlueLake/layout/_partial/post/reward.ejs","hash":"10b8c96d843651cb79425a09a8e98b88226aaac3","modified":1613241038712},{"_id":"themes/BlueLake/layout/_partial/post/share.ejs","hash":"c4afe1da14cca0eafd63dca60659a54dc290cdd7","modified":1613241038712},{"_id":"themes/BlueLake/layout/_partial/post/toc.ejs","hash":"019048b9be745fa094678303ef3f63607743998b","modified":1613241038712},{"_id":"themes/BlueLake/layout/_partial/post/title.ejs","hash":"a444db0ce077b443258b2b41e806c1135c8cb9b9","modified":1613241038712},{"_id":"themes/BlueLake/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1613241038712},{"_id":"themes/BlueLake/layout/_partial/post/views-count.ejs","hash":"52ebfe5a252909f8b15d9447be64da1645511277","modified":1613241038713},{"_id":"themes/BlueLake/source/css/_partial/archive.styl","hash":"0176062d48d82b7917876e19772f7b6430caaae7","modified":1613241038716},{"_id":"themes/BlueLake/source/css/_partial/about.styl","hash":"b936c5d0c9388c6b8097232bf92a9a46a196d6da","modified":1613241038716},{"_id":"themes/BlueLake/source/css/_partial/article.styl","hash":"b20545c2a8fce9840ead7a810a09afaf1c126f5b","modified":1613241038716},{"_id":"themes/BlueLake/source/css/_partial/footer.styl","hash":"5ae7745dc9b434eec4d0be97909dccfd413ba2a8","modified":1613241038716},{"_id":"themes/BlueLake/source/css/_partial/comment.styl","hash":"ac7cb71e18ec2c71159a28b94842e47222fd39bc","modified":1613241038716},{"_id":"themes/BlueLake/source/css/_partial/highlight.styl","hash":"d2e9dfbea2eaf42ebd6eb0ac127ed74f8bd35ca6","modified":1613241038717},{"_id":"themes/BlueLake/source/css/_partial/header.styl","hash":"b33995cbc911d86276b4d67d6d83a6b755ad2cce","modified":1613378515160},{"_id":"themes/BlueLake/source/css/_partial/iconfont.styl","hash":"77ef07c5dab7af0225e966bbd3a6bcf837a9e101","modified":1613241038717},{"_id":"themes/BlueLake/source/css/_partial/sidebar-aside.styl","hash":"ca9951a8b3a8b7a41de6b3c61aae8905afd30224","modified":1613381994021},{"_id":"themes/BlueLake/source/css/_partial/pagination.styl","hash":"684b498f319e5ebf13332c374f62a8ff5156b6f5","modified":1613241038717},{"_id":"themes/BlueLake/source/css/_partial/reward.styl","hash":"effc0f0fff4f0ff04e8a5b688793bb441973323a","modified":1613241038717},{"_id":"themes/BlueLake/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1613241038717},{"_id":"themes/BlueLake/source/css/_partial/sidebar-bottom.styl","hash":"d0b3f8607c3608a059ed9a884ba0eb7bc7603766","modified":1613241038717},{"_id":"themes/BlueLake/source/css/iconfont/iconfont.eot","hash":"d79ff9e8325c9c0d2db4ee6dadd2f82799169783","modified":1613241038718},{"_id":"themes/BlueLake/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1613241038718},{"_id":"themes/BlueLake/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1613241038718},{"_id":"themes/BlueLake/source/css/iconfont/iconfont.ttf","hash":"655ead66c5702e6e4a214c1473c78e450c49bfd7","modified":1613241038719},{"_id":"themes/BlueLake/source/css/iconfont/iconfont.svg","hash":"1a2fc918df7cd86721f376af104a389dd8d379ea","modified":1613241038719},{"_id":"themes/BlueLake/source/css/iconfont/iconfont.woff","hash":"29e34038a63a5a810f3af4a9c15dc1476d97fc93","modified":1613241038719},{"_id":"themes/BlueLake/source/css/iconfont/iconfont.woff2","hash":"09f1ae7849a0649f0bb8f46e128c97fcb59e3a4c","modified":1613241038719},{"_id":"themes/BlueLake/source/css/images/alipay.jpg","hash":"cfafbfa86d3223e5a7aa6025d4e78d26bdcf02af","modified":1613241038720},{"_id":"themes/BlueLake/source/css/images/alipay-pay.jpg","hash":"c1cdd36471a1197e31ee6309e979d1fb63daeb25","modified":1613241038719},{"_id":"themes/BlueLake/source/localshare/css/share.styl","hash":"21c948769783e4031d0d51c2a9f0ca7f387fb432","modified":1613241038727},{"_id":"themes/BlueLake/source/css/images/wechat.jpg","hash":"3a3f7518da72f35fbd8d0e3e3afa3018c4a434b2","modified":1613241038723},{"_id":"themes/BlueLake/source/css/images/wechat-pay.jpg","hash":"35f0f9859918d7b67d0a3279b110550667d97e8c","modified":1613241038723},{"_id":"themes/BlueLake/source/localshare/fonts/iconfont.svg","hash":"f0a1b849868a6bf351ff98dc3924a4e7254eb88b","modified":1613241038727},{"_id":"themes/BlueLake/source/localshare/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1613241038727},{"_id":"themes/BlueLake/source/localshare/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1613241038727},{"_id":"themes/BlueLake/source/localshare/js/qrcode.js","hash":"9caa0d9a9ba4409a3f77540a1b15a7617aeb28e8","modified":1613241038728},{"_id":"themes/BlueLake/source/localshare/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1613241038727},{"_id":"themes/BlueLake/source/localshare/js/social-share.js","hash":"9bfe34fe7691833e90815c5c862545bde581cf29","modified":1613241038728},{"_id":"themes/BlueLake/source/fancybox/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1613241038724},{"_id":"themes/BlueLake/source/js/jquery-3.4.1.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1613241038726},{"_id":"themes/BlueLake/source/css/images/banner.jpg","hash":"7e2fbdd5238cc2d374292f9d9b719dd7210b5bac","modified":1543786142137},{"_id":"themes/BlueLake/source/css/images/banner-dark.jpg","hash":"415265f85a302be2981b98ee54204391b1bbdfc7","modified":1628414739252},{"_id":"themes/BlueLake/source/css/.DS_Store","hash":"10afa0459387f890ff6778585df188dbd4296672","modified":1637895291687},{"_id":"themes/BlueLake/source/.DS_Store","hash":"ae227437a4b843ef8da25e230af459399ee51ce4","modified":1637907294413},{"_id":"themes/BlueLake/.DS_Store","hash":"e755af8d31201c95d6a31c65b6812da543a0e803","modified":1637920420153},{"_id":"themes/BlueLake/source/css/iconfont/SourceHanSansSC-Medium.otf","hash":"f481498954d09c4e70809b48bd46db50820b5e60","modified":1604446988000},{"_id":"source/.DS_Store","hash":"2c39b4de3dd7e919548bf6bce66350eb860cc28b","modified":1637923600114},{"_id":"source/_posts/2017-11-15-npm.md","hash":"db68f0d0e80f71a2fe3f5246ffd8f633fc051792","modified":1510761282000},{"_id":"source/_posts/2017-11-15-tmux.md","hash":"c46a94e2ebcaa2436000cbd9dde0b5a42dc345d7","modified":1546206012195},{"_id":"source/_posts/2017-11-20-1.md","hash":"0f0f06158e51dd152fc04dcc7c7393cf6e22040d","modified":1571446263027},{"_id":"source/_posts/2019-01-01-2018.md","hash":"eba55628cba9090eafd08094b6e9d4d59b55a4e2","modified":1546722119836},{"_id":"source/_posts/.DS_Store","hash":"1e654067998aa8eae99ad18596bbc08babbe7618","modified":1613310977179},{"_id":"themes/BlueLake/layout/.DS_Store","hash":"4f15b180c6bc1be2b37525f87b63492e624b7c54","modified":1637920542741},{"_id":"themes/BlueLake/layout/_partial/.DS_Store","hash":"e2766f6900843c339588ac62dffb94b1f5a3bfbf","modified":1613313016267},{"_id":"source/about/index.md","hash":"63a6486af6425deea00a29fb07deb68e0046e778","modified":1637913932486},{"_id":"source/categories/index.md","hash":"cf89a2fe344b402ebc37f5e749f642c57a85b6a4","modified":1613312077049},{"_id":"source/tags/index.md","hash":"e794c696795c057bf6284589df1a18089bf44ad3","modified":1613312085462},{"_id":"source/categorie/index.md","hash":"123a9feff5729f6f47ca24f0c616f02af24a4b97","modified":1613312916036},{"_id":"public/atom.xml","hash":"8aa515f3045a78bc11f2ebd918c4ff5fb2d00811","modified":1613312986017},{"_id":"public/sitemap.xml","hash":"a4dd1062f12fb3bff104d173a5b1c84248703e6b","modified":1637914262031},{"_id":"public/content.json","hash":"0b58ce2d58cd92e432d38d8953dae62b54b50332","modified":1613312986017},{"_id":"public/about/index.html","hash":"0f0496c7ae229954dd25694816d492bcfd6dd1fc","modified":1637920653918},{"_id":"public/categories/index.html","hash":"44b708ed264ed57a2646b0237ae5c20828a5eb16","modified":1613312986017},{"_id":"public/tags/index.html","hash":"8c95a0a086cf8e1131409a65996648a7f17f295c","modified":1613312986017},{"_id":"public/categorie/index.html","hash":"31f1c1cda16f0cd0664745ef0a8d80064834b1f6","modified":1613312986017},{"_id":"public/archives/index.html","hash":"9cf6aef34be2608070583423673d87b933a8924b","modified":1637920653918},{"_id":"public/archives/2017/index.html","hash":"366a203e45cb136a0e31b22f7b12d6cc71fc2d10","modified":1637920653918},{"_id":"public/archives/2017/11/index.html","hash":"3cf9142853a0635bb585965fe0aefb9d7b490bab","modified":1637920653918},{"_id":"public/categories/Machine-Learning/index.html","hash":"a63fef22de16c252400c831f286efec2965c8386","modified":1637920653918},{"_id":"public/index.html","hash":"7d2447e2c962fa8e6c74af06883b349b979b03a1","modified":1637920653918},{"_id":"public/tags/Learning-Theory/index.html","hash":"f9c9ff1c484a1625061faf0c86f92a8cdbb7be91","modified":1637920653918},{"_id":"public/2017/11/20/2017-11-20-1/index.html","hash":"54d51233aba7a4fc01938fb54821e0a0b5eb3d16","modified":1637920653918},{"_id":"public/css/iconfont/iconfont.ttf","hash":"655ead66c5702e6e4a214c1473c78e450c49bfd7","modified":1613312986017},{"_id":"public/css/iconfont/iconfont.eot","hash":"d79ff9e8325c9c0d2db4ee6dadd2f82799169783","modified":1613312986017},{"_id":"public/favicon.png","hash":"98f0ecbdcdc1a0e8e52f4d786cbc011e5e06fa65","modified":1613312986017},{"_id":"public/css/iconfont/iconfont.svg","hash":"1a2fc918df7cd86721f376af104a389dd8d379ea","modified":1613312986017},{"_id":"public/css/images/alipay-pay.jpg","hash":"c1cdd36471a1197e31ee6309e979d1fb63daeb25","modified":1613312986017},{"_id":"public/css/iconfont/iconfont.woff","hash":"29e34038a63a5a810f3af4a9c15dc1476d97fc93","modified":1613312986017},{"_id":"public/css/iconfont/iconfont.woff2","hash":"09f1ae7849a0649f0bb8f46e128c97fcb59e3a4c","modified":1613312986017},{"_id":"public/css/images/alipay.jpg","hash":"cfafbfa86d3223e5a7aa6025d4e78d26bdcf02af","modified":1613312986017},{"_id":"public/css/images/wechat.jpg","hash":"3a3f7518da72f35fbd8d0e3e3afa3018c4a434b2","modified":1613312986017},{"_id":"public/css/images/wechat-pay.jpg","hash":"35f0f9859918d7b67d0a3279b110550667d97e8c","modified":1613312986017},{"_id":"public/localshare/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1613312986017},{"_id":"public/localshare/fonts/iconfont.svg","hash":"f0a1b849868a6bf351ff98dc3924a4e7254eb88b","modified":1613312986017},{"_id":"public/localshare/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1613312986017},{"_id":"public/localshare/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1613312986017},{"_id":"public/css/images/banner.jpg","hash":"7e2fbdd5238cc2d374292f9d9b719dd7210b5bac","modified":1637907646451},{"_id":"public/css/images/banner-dark.jpg","hash":"0d2c406f1a9b3b74d6defda398239b0113737787","modified":1613312986017},{"_id":"public/fancybox/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1613312986017},{"_id":"public/js/script.js","hash":"b9441363ff8fe829d90295ea947960338a1a020a","modified":1613312986017},{"_id":"public/js/search.json.js","hash":"ec601a6db4c68a7d9c2d440951c6e26ebfbc1f3a","modified":1613312986017},{"_id":"public/localshare/css/share.css","hash":"34f53537ebcf2757b35a15a4a9473f2352486372","modified":1613312986017},{"_id":"public/localshare/js/social-share.js","hash":"9bfe34fe7691833e90815c5c862545bde581cf29","modified":1613312986017},{"_id":"public/css/style.css","hash":"d52c4c1a190b841ec18760b5fb8ab37c859016aa","modified":1613312986017},{"_id":"public/fancybox/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1613312986017},{"_id":"public/js/jquery-3.4.1.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1613312986017},{"_id":"public/localshare/js/qrcode.js","hash":"9caa0d9a9ba4409a3f77540a1b15a7617aeb28e8","modified":1613312986017},{"_id":"public/css/iconfont/SourceHanSansSC-Medium.otf","hash":"f481498954d09c4e70809b48bd46db50820b5e60","modified":1613312986017},{"_id":"themes/BlueLake/source/css/iconfont/.SourceHanSansSC-Medium.otf.icloud","hash":"643cb78508f39aae1e4c66abdf19ec79628a5fb5","modified":1604446988000},{"_id":"themes/BlueLake/source/localshare/.DS_Store","hash":"3cd0364a53bddae11d3ebae0f133d828fd870bd7","modified":1637907301128},{"_id":"themes/BlueLake/source/css/images/benjamin-davies-1151844-unsplash.jpg","hash":"bd921451dde28ce3e89fbf3056f65587b0ae4cd3","modified":1543018553497},{"_id":"themes/BlueLake/source/css/images/__banner.jpg","hash":"d23d92484e98adcbea7266ebfdfef7fb3fc42ee0","modified":1613241038722},{"_id":"themes/BlueLake/source/css/images/.DS_Store","hash":"20262d15385a7b719a17e567894fbae9ceff6f69","modified":1637908887758},{"_id":"themes/BlueLake/source/css/images/samuel-ferrara-136526-unsplash.jpg","hash":"ad1c2af3cd109ff0028399977fa3bf296cf9f807","modified":1543017830970},{"_id":"themes/BlueLake/source/css/images/banner.jpg.jpg","hash":"ad1c2af3cd109ff0028399977fa3bf296cf9f807","modified":1543017830970},{"_id":"themes/BlueLake/source/css/images/__bansssner.jpg","hash":"d23d92484e98adcbea7266ebfdfef7fb3fc42ee0","modified":1613241038722},{"_id":"public/css/images/banner.jpg.jpg","hash":"ad1c2af3cd109ff0028399977fa3bf296cf9f807","modified":1637907513589},{"_id":"themes/BlueLake/source/css/images/11banner.jpg","hash":"d23d92484e98adcbea7266ebfdfef7fb3fc42ee0","modified":1613241038722},{"_id":"public/css/images/11banner.jpg","hash":"d23d92484e98adcbea7266ebfdfef7fb3fc42ee0","modified":1637907646451},{"_id":"themes/BlueLake/source/css/images/c7ad985268e7144b588d7bf94eedb487_r.jpeg","hash":"415265f85a302be2981b98ee54204391b1bbdfc7","modified":1628414739252},{"_id":"themes/BlueLake/source/css/images/11banner-dark.jpg","hash":"0d2c406f1a9b3b74d6defda398239b0113737787","modified":1613241038721},{"_id":"public/css/images/11banner-dark.jpg","hash":"0d2c406f1a9b3b74d6defda398239b0113737787","modified":1637908626754},{"_id":"source/About/index.md","hash":"63a6486af6425deea00a29fb07deb68e0046e778","modified":1637913932789},{"_id":"public/About/index.html","hash":"8979c51336ea0b6fd0b5e4ff244ef7384c54f316","modified":1637914005855}],"Category":[{"name":"Linux","_id":"ckl5145al0002ysm6awizag8x"},{"name":"MISC","_id":"ckl5145ap0006ysm6bw83gyt6"},{"name":"Machine Learning","_id":"ckl5145ar000aysm6cjcmalz7"}],"Data":[],"Page":[{"title":"about","date":"2021-02-14T14:15:28.000Z","_content":"\n<center>\n    <h1>Pippa Passes</h1>\n    Robert Browning\n\nThe year’s at the spring,  \nAnd day's at the morn;  \nMorning's at seven;  \nThe hill-side’s dew-pearled;  \nThe lark's on the wing;  \nThe snail's on the thorn;  \nGod's in his heaven—  \nAll's right with the world!  \n</center>\n\n\n\n这是由Tsung维护的blog，主要记录一些心理学、行为经济学、认知科学、数据科学及各种技术、非技术的内容。\n\n如有兴趣，欢迎留言 : D\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2021-02-14 22:15:28\n---\n\n<center>\n    <h1>Pippa Passes</h1>\n    Robert Browning\n\nThe year’s at the spring,  \nAnd day's at the morn;  \nMorning's at seven;  \nThe hill-side’s dew-pearled;  \nThe lark's on the wing;  \nThe snail's on the thorn;  \nGod's in his heaven—  \nAll's right with the world!  \n</center>\n\n\n\n这是由Tsung维护的blog，主要记录一些心理学、行为经济学、认知科学、数据科学及各种技术、非技术的内容。\n\n如有兴趣，欢迎留言 : D\n","updated":"2021-11-26T08:05:32.789Z","path":"about/index.html","_id":"ckwg3q70x0000x6m6h0to47rv","comments":1,"layout":"page","content":"<center>\n    <h1>Pippa Passes</h1>\n    Robert Browning\n\n<p>The year’s at the spring,<br>And day’s at the morn;<br>Morning’s at seven;<br>The hill-side’s dew-pearled;<br>The lark’s on the wing;<br>The snail’s on the thorn;<br>God’s in his heaven—<br>All’s right with the world!  </p>\n</center>\n\n\n\n<p>这是由Tsung维护的blog，主要记录一些心理学、行为经济学、认知科学、数据科学及各种技术、非技术的内容。</p>\n<p>如有兴趣，欢迎留言 : D</p>\n","site":{"data":{}},"excerpt":"","more":"<center>\n    <h1>Pippa Passes</h1>\n    Robert Browning\n\n<p>The year’s at the spring,<br>And day’s at the morn;<br>Morning’s at seven;<br>The hill-side’s dew-pearled;<br>The lark’s on the wing;<br>The snail’s on the thorn;<br>God’s in his heaven—<br>All’s right with the world!  </p>\n</center>\n\n\n\n<p>这是由Tsung维护的blog，主要记录一些心理学、行为经济学、认知科学、数据科学及各种技术、非技术的内容。</p>\n<p>如有兴趣，欢迎留言 : D</p>\n"}],"Post":[{"title":"机器学习理论(1) 学习理论框架","author":"Tsung","date":"2017-11-20T00:00:00.000Z","slug":"2017-11-20-1","description":"","thumbnail":"","_content":"\n学习的基础是可学习性。可学习性包括三个方面：是否是可计算的；计算的复杂度如何；算法如何实现。在机器学习领域有一整套严格的形式化语言来描述这些问题。作为机器学习理论系列的第一篇，这里主要梳理了学习的概念、学习的分类、学习方法的大致框架等内容。\n\n<!--more-->\n\n# 1. 什么是学习\n\n学习一词是我们再熟悉不过的了，从小时候总是被提及的好好学习，天天向上，到政治课本上的学习型社会，终身学习；从MOOC，远程教育等愈发多样的学习形式，再到引广泛争论的学习无用论，学习的概念充斥于我们的生活。\n\n学习也同样是心理学(Psychology)，认知神经科学(Cognitive Neuroscience, CN)，计算机科学(Computer Science, CS)等领域所关心的课题。个体的学习能力发展，学习的方法技巧，学习的分子机制，机器学习的原理与应用等等都是当今教育、科学领域的热门话题。\n\n学习之所以如此为家长们所汲汲，为大众所乐道，大概是因为学习总是与智慧的评价，解决问题的能力，竞争中所能取得的优势等相关联，看起来大概是有助于提升我们的适应与生存能力。\n\n我们对于学习是如此熟悉，学习对于我们而言却又如此陌生，我们对于学习的生理机制也还仅略知皮毛，对于machine learning的实现也还抱有一些疑问，甚至于我们没有一个统一而确切的概念去描述它。在Psychology和NS中，学习与记忆是一对经常被一起提到的概念，对于学习与记忆间的关系也常有着如下的定义：\n\n学习(Learning)是获取新信息的过程，学习的结果就是记忆(Memory)。学习与记忆可以假设为如下三个主要阶段：\n\n1. 编码(encoding): 对输入的信息进行处理与储存，编码又可以分为获取(acquision)和巩固(consolidation)这两个过程。获取指的是对感觉通路和感觉分析阶段的输入信息进行登记；巩固是生成一个虽时间推移而增强的表征。\n2. 存储(storage): 是获取和巩固的结果，代表了信息的长久记录。\n3. 提取(retrieval): 是通过利用所存储的信息创建意识表征或执行习得的行为，如自动化动作。\n\n这似乎很符合我们的经验：我们将经验与知识保存为记忆，在新的情景下调用记忆处理新的问题，比如当学习过how to make a good cup of tea的protocol后，再次需要make tea的时候，按照记忆中的步骤一步步完成就可以了；我们将所确认过的骚扰电话存入一个数据库，当遇到一个新号码时，只需将这个号码和数据库中的号码一一比对，即可判断是不是骚扰电话。\n\n虽然这种基于记忆的学习方式看起来很有用，但有一些问题我们却不能忽视：当我们认识一个新朋友时，我们会记住他的面孔，口音，衣着等等；再次见面时，也许他的衣着变了，发型变了，虽然他和我们记忆中的表征有些不一样了，但我们还是会把他当作认识的那个人；即使说话习惯从广州白话变成了上海闲话，或许我们还是能够认出他来；但如果是从一个文艺青年变成了中二少年。。那个，不好意思，请问您是那位。。\n\n这提示我们：需要有多大的不同我们才会判断新出现的目标与我们记忆中的原型是不一致的呢？换言之，我们是通过什么标准来进行分类判别的呢？再把问题拓展一点，学习貌似不单单是与记忆做比较，成功的学习应当能够从个例出发进行泛化，在一定范围内对于目标的变异具有适应性，甚至在具有相似特征的对象间进行迁移：如果事先知道青的橘子，苹果，杏都很酸，那么在第一次遇见青的葡萄时，应当也可以假设，青的葡萄也是酸的；如果学习不具有这种能力的话，那么假如男朋友换了个发型就不认识了，那该是个多么悲伤的故事: )\n\n更何况，记忆本身的维持也是具有不同程度的区别，记忆的熟悉性与再认在功能与机制上可能也是分离的，有些事情我们明明已经记不大清了，但仍会影响到我们对于新事物的决策，如何解释这一现象，它们对于学习分别有何影响，也是需要我们去思考的。从学习的角度出发，或许记忆本就是学习的一种副产品，记忆的价值，进化中的动力，也几乎都是通过学习来体现的。进一步从学习机制的角度考虑，记忆也许仅仅是feedforward network学习过程中训练出的firing sequence的一种体现，[Brunel][1]的一些研究就为这一看法提供了支持。还有更多的一些可能，这里暂不展开讨论。\n\n# 2. 归纳偏置与credit assignment problem\n\n对于学习，一个概括性，不够全面，非操作性的定义是，学习是一个将经验转化为专业技能或知识的过程。操作性的定义大概在学习的一般模型中可以看到，这里我们先来讨论两个例子。\n\n第一个例子是B. F. Skinner著名的实验，迷信的鸽子。Skinner在鸽笼中放置了定时投喂装置，当食物第一次送达时，鸽子们处于不同的活动状态，如转动头部，转圈等。鸽子们误以为食物的到来与自己的动作有关，于是食物的到来强化了这种动作，为了得到更多的食物，鸽子们不断重复第一次食物送来时的动作状态。\n\n第二个例子是老鼠的怯饵效应。Garcia & Koelling (1996)发现，当在大鼠进食后给予电击，大鼠并没有将电击与进食联系起来，即使将进食后的电击重复多次，大鼠仍然倾向于继续进食。当然，这可能仅仅因为大鼠觉得电就电好了，反正我要吃饱。。于是进一步地，研究者在大鼠的食物中混杂药物，导致大鼠食用食物后反胃呕吐，大鼠会避免再次进食；而如果在大鼠进食有毒食物的同时给予声音刺激，大鼠也并没有表现出如再次面对有毒食物时的害怕趋避表现。这似乎表明，大鼠具有某种“先验知识”，能够理解进食与电击，声音与反胃间并不存在直接的因果关系。\n\n鸽子与大鼠间行为差异的关键在于，可能存在的先验知识的引入使得学习机制出现了偏差，鸽子无法区分诸多线索中哪一个才是真正与反馈的出现存在因果关系，从而错误地将某一线索与反馈联系起来了；大鼠拥有关于食物、点击、不良反应、声音等的先验知识，在学习的过程中，这些先验知识使得学习的方向发生了偏差，从而获取有先验知识约束的更为准确的知识，这种现象被称做归纳偏置。\n\n在之后对于偏差和复杂度的讨论中我们可以证明，归纳偏置对于一个成功的算法而言是必不可少的部分。先验假设越强，就越容易从样本中进行可靠的学习；然而同时，过强的先验假设也会牺牲学习的灵活性。\n\n与鸽子与大鼠的经历类似，在生活中，我们常常会同时接受很多种刺激，如何将真正有价值的线索-反馈对分离出来，并针对某一特定反馈，为不同的线索的分配价值，这样的credit assignment problem是学习过程不可回避的问题。不幸的是，很多时候我们也并没有比鸽子做得更好，关于credit assignment problem更多的细节或许会在一个新的专题来进行讨论。\n\n# 3. 学习的种类\n\n根据不同的分类标准，学习有多种分类方法。就学习理论的研究而言，可能会更多地倾向于从学习与环境的关系，学习的主动性，学习的形式这三个角度来进行分类。\n\n## 3.1 学习与环境的关系\n\n如果把学习过程抽象为“利用经验获取技能的过程”，经验即是学习的对象，通过从经验中获取“技能”，解决新的问题即是学习的结果。在不同的情景下我们能够利用的经验也是不同的。根据学习时能够从经验中获取的信息，可以将学习分为监督学习，非监督学习，和强化学习三类。\n\n### 监督学习(Supervised Learning)\n\n监督学习(Unsupervised Learning)顾名思义指的是在学习的过程中存在“监督者”对学习的结果，即提供正确的输入输出信息供学习器（脑或机器）学习。如对于急性昏迷病人的预后判断，我们希望可以通过病人的脑电数据检测预测病人的预后（能够转醒或进入持续性植物状态，PVS）。那么我们就可以事先获得已知预后的病人的资料，并将脑电数据与预后状况配对，交给学习器进行学习。\n\n在这种情况下经验中包含了有关学习对象的完整的信息，通过已知正确的输入与输出对，学习所提供样本的特征，从而对与训练数据相似的新的数据进行分类，或检测其中的异常值。在上面的例子中，如果学习器学会了脑电数据与预后间的关系，那么就可以利用这种关系预测新的病人的预后了。\n\n### 非监督学习(Unsupervised Learning)\n\n在非监督学习(Unsupervised Learning)中，经验没有提供学习对象额外的信息，用于学习的对象即训练数据与学习器在学习后需要接触到的实际数据间没有区别。同样以急性昏迷病人的预后判断为例，我们只是单纯地提供给学习器病人的脑电数据，学习器需要自己在这些数据中获取信息，发现规律，通常的结果是，学习器可以从这些数据中提取出概括的信息，即降维，如找出脑电信号中的主要成分，或是进行聚类，将相似的数据归为一类，如在理想情况下，学习器可能会自然地将病人分为两类，恰好对应预后转醒和PVS。\n\n### 强化学习(Reinforcement learning, RL)\n\n强化学习(Reinforcement Learning, RL)大致是介于监督学习与非监督学习之间，虽然没有一个监督者能够告诉强化学习的学习器正确的答案，但能够对学习器的选择给予反馈，David Silver（main programmer on the Go team at DeepMind Google）在[UCL的课程][2]中对RL有很好的介绍。\n\n强化学习是基于奖励假设(Reward Hypothesis)的一种学习过程：\n\n> **Reward Hypothesis:** All goals can be described by the maximisation of expected cumulative reward\n\n在强化学习中，通常会有一个预设的目标，如在试图在围棋对弈中取胜，围棋中的每一步行子并不会有绝对的正误之分，但却会对最终能否取胜产生影响，我们可以分析每一步对于最终获胜的贡献程度而选择最佳的落子方式。\n\n与其他的学习方式相比，RL具有如下特征：\n\n1. 不存在监督者，只有奖励信号\n2. 反馈会延迟，而非即时对应，一个动作的反馈信号可能会很久后到来\n3. 时间很重要，RL是一个动态的过程，这一过程经常被马尔科夫决策过程所描述，动作-状态对是序列发生的，并不能通过i.i.d数据进行学习；结合第二点，动作的反馈可能有长时与短时之分，短时获益的选择可能会导致一段时间后的损失，眼下并不能获益的行为可能最有助于达到最终的目标。\n4. 如上所述，学习器所获得的信号并非是固定的，而是由其自身的行为决定。某一时刻下学习器的选择会决定之后的状态。\n\n我们所能够遇到的场景通常也都与RL所假设的一致，总体上看是在探索未知领域与遵从已有知识间找到平衡。这种目标导向的学习方式相较于监督学习与非监督学习也更加灵活，也被认为是人类最为主要的学习方式之一。\n\n## 3.2 主动学习器与被动学习器\n\n学习器可以根据与获取训练数据的方式分为主动学习器和被动学习器。\n\n主动学习器会在学习时通过提问或实验的方式与环境交互，如一个学习器在学习标记垃圾邮件时，会主动挑选出一些邮件要求用户标记，从而提高对“垃圾邮件的理解”。\n\n被动学习器则完全依据事先提供的信息进行学习。\n\n## 3.3 在线学习与批量学习\n\n批量学习指一次性处理大量的数据后获取技能；在线学习指在新数据到来后，立即考虑新数据的影响并对学习结果进行更新，一般而言，这样的学习结果更加有效。而从人类的学习与分析表现上看，应当是具有实现类似于在线学习的能力。\n\n# 4. 学习的方法\n\n关于学习理论，有许多不同的流派持有不同的看法。这里就产生式分类和判别式分类，频率派与贝叶斯派的基本方法进行简单的介绍。\n\n## 4.1 生成的分类与识别的分类\n\n考虑如果已知观测对象是模式$\\boldsymbol{x}$，如这是急性昏迷病人的脑电模式，使得分类类别$y$（预后：转醒，PVS）在模式$\\boldsymbol{x}$下的条件概率$p(y\\mid\\boldsymbol{x})$最大的的类别$\\hat{y}$，就是这一病人相匹配的预后类别。\n$$\\hat{y}=\\mathrm{argmax\\; }p\\_y(y\\mid\\boldsymbol{x})$$\n这里”$\\mathrm{argmax}$ “是取最大值时的参数的意思。在模式识别里条件概率$p(y\\mid\\boldsymbol{x})$常被称作后验概率。应用训练集直接对后验概率$p(y\\mid\\boldsymbol{x})$进行学习的过程，称为判别式分析。\n\n此外，还可以把后验概率$p(y\\mid\\boldsymbol{x})$表示为$y$的函数\n$$\np(y\\mid\\boldsymbol{x})=\\frac{p(\\boldsymbol{x},y)}{p(\\boldsymbol{x})}\\propto{p(\\boldsymbol{x},y)}\n$$\n根据上式，我们可以发现模式$\\boldsymbol{x}$和类别$y$的联合概率$p(\\boldsymbol{x},y)$与后验概率$p(y|\\boldsymbol{x})$是成正比的。因此，我们可以通过使联合概率$p(\\boldsymbol{x},y)$达到最大值的方法，得到使后验概率$p(y\\mid\\boldsymbol{x})$达到最大值的类别$\\hat{y}$。\n$$\n\\hat{y}=\\mathrm{argmax\\;}p\\_y(\\boldsymbol{x},y)\n$$\n在模式识别力，联合概率$p(\\boldsymbol{x},y)$也被成为数据生成概率，通过预测数据生成概率$p(\\boldsymbol{x},y)$来进行模式识别的分类方法，称为生成的分类。\n\n一般而言，解决一个特殊问题所需的信息小于解决这个特殊问题所属的一般问题的信息。因为生成概率与后验概率的关系为：\n$$\np(y\\mid\\boldsymbol{x})=\\frac{p(\\boldsymbol{x},y)}{p(\\boldsymbol{x})}=\\frac{p(\\boldsymbol{x},y)}{\\Sigma\\_yp(\\boldsymbol{x},y)}\n$$\n在已知生成概率$p(\\boldsymbol{x},y)$的情况下可以很容易地得到后验概率$p(y\\mid\\boldsymbol{x})$，而已知后验概率$p(y\\mid\\boldsymbol{x})$的情况下却难以得到生成概率$p(\\boldsymbol{x},y)$。\n\n在具有一定先验知识的情况下，可以事先得到生成概率$p(\\boldsymbol{x},y)$，在先验概率的约束下，相当于具有了归纳偏置的能力，能对推断结果有更好的限制，相对而言生成的分类是更好的方式；而在没有先验知识，不知道生成概率$p(\\boldsymbol{x},y)$的情况下，则仅能够通过后验概率$p(y\\mid\\boldsymbol{x})$使用识别的分类进行推断了。\n\n## 4.2 统计概率和朴素贝叶斯\n\n统计概率的学习方法将对某一模式的描述看作是学习的结果；学习的目的是通过训练集如$\\mathcal{D}$，得到对包含参数$\\boldsymbol{\\theta}$的模式$q(\\boldsymbol{x},y;\\boldsymbol{\\theta})$的精确描述。\n$$\n\\max\\_\\theta\\prod\\_{i=1}^{y}q(\\boldsymbol{x}\\_i,y\\_i;\\boldsymbol{\\theta})\n$$\n朴素贝叶斯方法则将从模式$\\boldsymbol{\\theta}$产生的角度出发，将其看作是概率变量；依据贝叶斯定理，就可以通过求得的后验概率$p(\\boldsymbol{\\theta}\\mid\\mathcal{D})$估计先验概率$p(\\boldsymbol{\\theta})$。\n$$\np(\\boldsymbol{\\theta}\\mid\\mathcal{D})=\\frac{p(\\mathcal{D}\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathcal{D})}=\\frac{\\prod\\_{i=1}^nq(\\boldsymbol{x}\\_i,y\\_i\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{\\int\\prod\\_{i=1}^nq(\\boldsymbol{x}\\_i,y\\_i\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\mathrm{d}\\boldsymbol{\\theta}}\n$$\n\n# 5. 学习的理论框架\n\n## 5.1 学习器的输入\n\n### 领域集(domain set)\n\n领域集即我们希望学习的对象的总体，可以是任意的集合$\\mathcal{X}$。领域集中的元素通常可以用描述了其特征的向量表示，从心理学研究问题的角度出发，这些特征就是有关研究对象的因素，每个因素又存在不同的水平。如对于木瓜而言，其具有的特征有颜色，软硬程度等，对于颜色特征而言又有绿色，黄色等水平。在围棋对弈中，可以落子的位置构成了当前状态下的领域集。\n\n领域集中的元素也叫做实例，因此$\\mathcal{X}$也被叫做实例空间。\n\n### 标签集(label set)\n\n标签是指我们希望通过学习得到的有关实例的信息，标签集$\\mathcal{Y}$即是所有标签的集合。就木瓜的例子而言，我们会希望通过对于木瓜特征的学习，辨别木瓜好不好吃，因此给予木瓜的标签可以是好吃（为方便起见，用$1$表示）和不好吃（用$0$表示），因此对于木瓜的领域集$\\mathcal{X}$而言，其标签集$\\mathcal{Y}$为$\\{1,0\\}$。\n\n### 训练数据(training data)\n\n为了使得学习器学习到有关领域集的知识，通常我们都需要获得该领域集中的一些样本供学习器学习。训练数据至少应包含领域集内实例的部分特征，在监督学习情况下，训练数据还应包含实例所对应的标签，即训练数据以$\\mathcal{X}\\times\\mathcal{Y}$的形式成对出现，训练数据可以表示为训练集$S=\\{(X\\_1,Y\\_1),\\cdots,(x\\_m,y\\_m)\\}$这样一个有限的序列。\n\n## 5.2 学习器的输出\n\n学习器学习的目的是从训练数据中学会适用于训练数据所属领域集的知识，即可以给未加标签的实例加上预测的标签，这样的过程可以用一个描述从$\\mathcal{X}$到$\\mathcal{Y}$映射的函数表示，$h:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，这样的映射被称为预测规则(prediction rule)，这样的函数也可以被称为预测器(predictor)，假设(hypothesis)或分类器(classifier)。\n\n通常我们用$A(S)$来表示学习算法$A$在给定的训练序列$S$下返回的假设。\n\n## 5.3 获取训练数据\n\n从领域集中抽取合适的样本实际上是个很复杂的问题，通常我们会希望抽取的样本能与领域集独立同分布(i.i.d)，即训练数据$\\mathcal{S}$亦从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中抽取，服从$\\mathcal{D}$的分布。\n\n我们假设，对领域集而言，实例特征与标签间存在正确的映射关系$f:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，对于任意的$i$均有$y\\_i=f(x\\_i)$。学习器通过算法$A$学习训练数据，以正确的映射关系$f(x)$为目标，寻找$\\mathcal{X}$与$\\mathcal{Y}$间的映射关系，为训练数据赋予正确的标签，最终产生学习到的映射关系$h(x)$。\n\n## 5.4 评估学习结果\n\n分类器的误差被定义为，从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中随机抽取一个样本$x$，使得$h(x)\\boldsymbol\\ne f(x)$的概率。\n\n$h:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$的错误率被定义为$\\newcommand{\\eqdef}{\\overset{\\mathrm{def}}{=}}$ \n\n$$\nL_{\\mathcal{D},f}\\eqdef\\mathbb{P}\\_{x\\sim\\mathcal{D}}[h(x)\\ne f(x)]\\eqdef\\mathcal{D}(\\{x:h(x)\\neq f(x)\\})\n$$\n$L\\_{\\mathcal{D},f}(h)$也称为泛化误差，真实误差，或损失函数(cost function)，其下标$(\\mathcal{D},f)$表明误差的测量是基于概率分布$\\mathcal{D}$和正确标记函数$f(x)$进行的。在接下来对于PAC可学习性的讨论中，还会讨论这种损失的其他的可能形式。\n\n# 6. 经验风险最小化\n\n## 6.1 经验风险\n\n学习的过程是从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中获取训练集$S$，我们希望学习器能通过某种算法得到一个预测器$h\\_S:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，使得关于分布$\\mathcal{D}$和正确标记函数$f(x)$的预测误差最小化。\n\n由于我们并不知道分布$\\mathcal{D}$和正确标记函数$f(x)$究竟是什么样的，因此我们需要借助学习器在训练样本中的误差来对犯错的风险进行估计：\n$$\nL\\_S(h)\\eqdef\\frac{\\vert\\{i\\in[m]:h(x\\_i)\\ne y\\_i\\}\\vert}{m}\n$$\n其中，$[m]=\\{1,\\cdots,m\\}$\n\n这种从训练数据中得到的对风险的估计被称为经验风险或经验误差。由于我们假设训练集能够代表领域集的特征，因此，这种从训练集估计$\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$映射并以经验风险代表估计的真实风险的做法是可以接受的。这种寻求拥有最小经验风险的过程称为经验风险最小化(Empirical Risk Minization，ERM)。\n\n## 6.2 过拟合\n\n由于训练数据具有有限的数据量，如果不考虑寻找到的规则$h(x)$的复杂度，一定能够找到一种方法，将训练集中$\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$的关系拟合得很好。一个极端的例子是，如果学习过程变成了单纯的记忆，那么$L\\_\\mathcal{D}(h\\_S)$一定会等于$0$。\n\n通常过拟合发生时会导致，对训练数据拟合得非常好，但在训练数据之外表现得非常糟糕。\n\n# 7. 考虑归纳偏置的经验风险最小化\n\n## 7.1 归纳偏置的影响\n\n过拟合产生的一个重要原因在于，学习器对于真实的映射关系$f(x)$没有任何先验的概念，即不知道$f(x)$应该有的样子，$f(x)$本身应该有的性质无法对本应与$f(x)$一致的$h(x)$产生限制，$h(x)$为了适应数据而可以不考虑复杂度变得非常奇怪。\n\n又或者说，如果仅仅知道晚饭吃饱了而去推测晚上吃了什么，那么无论是中餐、西餐、日料、压缩饼干甚至是某些奇奇怪怪的东西都可以满足这一要求，但如果事先知道这是个中国人，而且是南方人，那么猜测的范围就可以大大缩小了。\n\n因此，在学习器接触训练集开始学习之前，就可以通过基于所学学习问题的先验知识，将基于训练集$S$的可能的选择器$h(x)$的范围限制于一个集合$\\mathcal{H}$中，这样的一个集合被称为假设类。在ERM准则下，假设类利用ERM规则进一步选择出合适的预测器$h\\boldsymbol\\in\\mathcal{H}$，这一过程可用如下形式表示\n$$\nERM\\_\\mathcal{H}(S)\\in\\mathrm{argmax\\;}\\_{h\\in\\mathcal{H}}L\\_S(h)\n$$\n接下来将会证明，为什么基于此假设类的$\\mathrm{ERM}_\\mathcal{H}$能够保证不过拟合，以及选择哪种假设类$\\mathrm{ERM}\\_\\mathcal{H}$不会导致过拟合。\n\n## 7.2 真实风险的来源\n\n### 置信参数(confidence parameter)\n\n由于学习器仅能通过训练集$S$学习在$\\mathcal{D}$上分布的领域集$\\mathcal{X}$，因此$S$与$\\mathcal{X}$分布的差异会导致真实误差$L\\_{\\mathcal{H},f}$的增加。为保证学习器所学习到的$h(x)$的准确性，需要保证$S$中的训练样本满足独立同分布(i.i.d.)假设，都是从$\\mathcal{D}$中独立同分布地抽取的，记为$S\\boldsymbol\\sim\\mathcal{D}^m$，其中$m$为$S$的势，即为在分布$\\mathcal{D}$下，独立同分布采样$S\\boldsymbol=z\\_1,\\cdots,z\\_m$。\n\n然而实际上，我们难以保证抽样的独立同分布，即使一学期仅有三次点名，一学期也仅skip三次课，也是有可能全部赶上点名的: )\n\n因此，我们将采样到不能代表领域集$\\mathcal{X}$的训练集$S$的概率表示为$\\delta$，并将$1-\\delta$称为该预测的置信参数(confidence parameter)\n\n### 精度参数(accuracy parameter)\n\n同样的，我们也无法保证$h(x)$所给出的预测结果的正确性，此时，我们使用精度参数(accuracy parameter)$\\varepsilon$来评价$h(x)$预测的质量，如果$L\\_{\\mathcal{D},f}>\\varepsilon$，那么$h(x)$就是一种失败的预测；如果$L\\_{\\mathcal{D},f}{\\leq}\\varepsilon$，那么就认为$h(x)$给出了一个误差可以接受的预测。\n\n## 7.3 学习的可靠性\n\n对于$S\\sim\\mathcal{D}^m$的训练集来说，设$\\mathcal{H}\\_B$是失败的假设集合，那么有：\n$$\n\\mathcal{H}\\_B=\\{h\\in\\mathcal{H}:L\\_{\\mathcal{D},f}(h)\\>\\varepsilon\\}\n$$\n设集合$M$是一个误导集，\n$$\nM=\\{S\\mid\\_x \\exists \\:h\\in\\mathcal{H}\\_B,L\\_S(h)=0\\}\n$$\n\n即$M$中的假设对于领域集来说是失败的预测，但对于训练集来说，则是符合精度参数要求的。即$M$描述了过拟合的$h(x)$。\n\n[1]:\thttp://utmemoryclub.com/wp-content/uploads/2013/07/brunel2016.pdf\n[2]:\thttp://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html","source":"_posts/2017-11-20-1.md","raw":"---\ntitle: 机器学习理论(1) 学习理论框架\nauthor: Tsung\ndate: '2017-11-20'\nslug: machine_learning_theory_1\ncategories:\n  - Machine Learning\ntags:\n  - Learning Theory\ndescription: ''\nthumbnail: ''\n---\n\n学习的基础是可学习性。可学习性包括三个方面：是否是可计算的；计算的复杂度如何；算法如何实现。在机器学习领域有一整套严格的形式化语言来描述这些问题。作为机器学习理论系列的第一篇，这里主要梳理了学习的概念、学习的分类、学习方法的大致框架等内容。\n\n<!--more-->\n\n# 1. 什么是学习\n\n学习一词是我们再熟悉不过的了，从小时候总是被提及的好好学习，天天向上，到政治课本上的学习型社会，终身学习；从MOOC，远程教育等愈发多样的学习形式，再到引广泛争论的学习无用论，学习的概念充斥于我们的生活。\n\n学习也同样是心理学(Psychology)，认知神经科学(Cognitive Neuroscience, CN)，计算机科学(Computer Science, CS)等领域所关心的课题。个体的学习能力发展，学习的方法技巧，学习的分子机制，机器学习的原理与应用等等都是当今教育、科学领域的热门话题。\n\n学习之所以如此为家长们所汲汲，为大众所乐道，大概是因为学习总是与智慧的评价，解决问题的能力，竞争中所能取得的优势等相关联，看起来大概是有助于提升我们的适应与生存能力。\n\n我们对于学习是如此熟悉，学习对于我们而言却又如此陌生，我们对于学习的生理机制也还仅略知皮毛，对于machine learning的实现也还抱有一些疑问，甚至于我们没有一个统一而确切的概念去描述它。在Psychology和NS中，学习与记忆是一对经常被一起提到的概念，对于学习与记忆间的关系也常有着如下的定义：\n\n学习(Learning)是获取新信息的过程，学习的结果就是记忆(Memory)。学习与记忆可以假设为如下三个主要阶段：\n\n1. 编码(encoding): 对输入的信息进行处理与储存，编码又可以分为获取(acquision)和巩固(consolidation)这两个过程。获取指的是对感觉通路和感觉分析阶段的输入信息进行登记；巩固是生成一个虽时间推移而增强的表征。\n2. 存储(storage): 是获取和巩固的结果，代表了信息的长久记录。\n3. 提取(retrieval): 是通过利用所存储的信息创建意识表征或执行习得的行为，如自动化动作。\n\n这似乎很符合我们的经验：我们将经验与知识保存为记忆，在新的情景下调用记忆处理新的问题，比如当学习过how to make a good cup of tea的protocol后，再次需要make tea的时候，按照记忆中的步骤一步步完成就可以了；我们将所确认过的骚扰电话存入一个数据库，当遇到一个新号码时，只需将这个号码和数据库中的号码一一比对，即可判断是不是骚扰电话。\n\n虽然这种基于记忆的学习方式看起来很有用，但有一些问题我们却不能忽视：当我们认识一个新朋友时，我们会记住他的面孔，口音，衣着等等；再次见面时，也许他的衣着变了，发型变了，虽然他和我们记忆中的表征有些不一样了，但我们还是会把他当作认识的那个人；即使说话习惯从广州白话变成了上海闲话，或许我们还是能够认出他来；但如果是从一个文艺青年变成了中二少年。。那个，不好意思，请问您是那位。。\n\n这提示我们：需要有多大的不同我们才会判断新出现的目标与我们记忆中的原型是不一致的呢？换言之，我们是通过什么标准来进行分类判别的呢？再把问题拓展一点，学习貌似不单单是与记忆做比较，成功的学习应当能够从个例出发进行泛化，在一定范围内对于目标的变异具有适应性，甚至在具有相似特征的对象间进行迁移：如果事先知道青的橘子，苹果，杏都很酸，那么在第一次遇见青的葡萄时，应当也可以假设，青的葡萄也是酸的；如果学习不具有这种能力的话，那么假如男朋友换了个发型就不认识了，那该是个多么悲伤的故事: )\n\n更何况，记忆本身的维持也是具有不同程度的区别，记忆的熟悉性与再认在功能与机制上可能也是分离的，有些事情我们明明已经记不大清了，但仍会影响到我们对于新事物的决策，如何解释这一现象，它们对于学习分别有何影响，也是需要我们去思考的。从学习的角度出发，或许记忆本就是学习的一种副产品，记忆的价值，进化中的动力，也几乎都是通过学习来体现的。进一步从学习机制的角度考虑，记忆也许仅仅是feedforward network学习过程中训练出的firing sequence的一种体现，[Brunel][1]的一些研究就为这一看法提供了支持。还有更多的一些可能，这里暂不展开讨论。\n\n# 2. 归纳偏置与credit assignment problem\n\n对于学习，一个概括性，不够全面，非操作性的定义是，学习是一个将经验转化为专业技能或知识的过程。操作性的定义大概在学习的一般模型中可以看到，这里我们先来讨论两个例子。\n\n第一个例子是B. F. Skinner著名的实验，迷信的鸽子。Skinner在鸽笼中放置了定时投喂装置，当食物第一次送达时，鸽子们处于不同的活动状态，如转动头部，转圈等。鸽子们误以为食物的到来与自己的动作有关，于是食物的到来强化了这种动作，为了得到更多的食物，鸽子们不断重复第一次食物送来时的动作状态。\n\n第二个例子是老鼠的怯饵效应。Garcia & Koelling (1996)发现，当在大鼠进食后给予电击，大鼠并没有将电击与进食联系起来，即使将进食后的电击重复多次，大鼠仍然倾向于继续进食。当然，这可能仅仅因为大鼠觉得电就电好了，反正我要吃饱。。于是进一步地，研究者在大鼠的食物中混杂药物，导致大鼠食用食物后反胃呕吐，大鼠会避免再次进食；而如果在大鼠进食有毒食物的同时给予声音刺激，大鼠也并没有表现出如再次面对有毒食物时的害怕趋避表现。这似乎表明，大鼠具有某种“先验知识”，能够理解进食与电击，声音与反胃间并不存在直接的因果关系。\n\n鸽子与大鼠间行为差异的关键在于，可能存在的先验知识的引入使得学习机制出现了偏差，鸽子无法区分诸多线索中哪一个才是真正与反馈的出现存在因果关系，从而错误地将某一线索与反馈联系起来了；大鼠拥有关于食物、点击、不良反应、声音等的先验知识，在学习的过程中，这些先验知识使得学习的方向发生了偏差，从而获取有先验知识约束的更为准确的知识，这种现象被称做归纳偏置。\n\n在之后对于偏差和复杂度的讨论中我们可以证明，归纳偏置对于一个成功的算法而言是必不可少的部分。先验假设越强，就越容易从样本中进行可靠的学习；然而同时，过强的先验假设也会牺牲学习的灵活性。\n\n与鸽子与大鼠的经历类似，在生活中，我们常常会同时接受很多种刺激，如何将真正有价值的线索-反馈对分离出来，并针对某一特定反馈，为不同的线索的分配价值，这样的credit assignment problem是学习过程不可回避的问题。不幸的是，很多时候我们也并没有比鸽子做得更好，关于credit assignment problem更多的细节或许会在一个新的专题来进行讨论。\n\n# 3. 学习的种类\n\n根据不同的分类标准，学习有多种分类方法。就学习理论的研究而言，可能会更多地倾向于从学习与环境的关系，学习的主动性，学习的形式这三个角度来进行分类。\n\n## 3.1 学习与环境的关系\n\n如果把学习过程抽象为“利用经验获取技能的过程”，经验即是学习的对象，通过从经验中获取“技能”，解决新的问题即是学习的结果。在不同的情景下我们能够利用的经验也是不同的。根据学习时能够从经验中获取的信息，可以将学习分为监督学习，非监督学习，和强化学习三类。\n\n### 监督学习(Supervised Learning)\n\n监督学习(Unsupervised Learning)顾名思义指的是在学习的过程中存在“监督者”对学习的结果，即提供正确的输入输出信息供学习器（脑或机器）学习。如对于急性昏迷病人的预后判断，我们希望可以通过病人的脑电数据检测预测病人的预后（能够转醒或进入持续性植物状态，PVS）。那么我们就可以事先获得已知预后的病人的资料，并将脑电数据与预后状况配对，交给学习器进行学习。\n\n在这种情况下经验中包含了有关学习对象的完整的信息，通过已知正确的输入与输出对，学习所提供样本的特征，从而对与训练数据相似的新的数据进行分类，或检测其中的异常值。在上面的例子中，如果学习器学会了脑电数据与预后间的关系，那么就可以利用这种关系预测新的病人的预后了。\n\n### 非监督学习(Unsupervised Learning)\n\n在非监督学习(Unsupervised Learning)中，经验没有提供学习对象额外的信息，用于学习的对象即训练数据与学习器在学习后需要接触到的实际数据间没有区别。同样以急性昏迷病人的预后判断为例，我们只是单纯地提供给学习器病人的脑电数据，学习器需要自己在这些数据中获取信息，发现规律，通常的结果是，学习器可以从这些数据中提取出概括的信息，即降维，如找出脑电信号中的主要成分，或是进行聚类，将相似的数据归为一类，如在理想情况下，学习器可能会自然地将病人分为两类，恰好对应预后转醒和PVS。\n\n### 强化学习(Reinforcement learning, RL)\n\n强化学习(Reinforcement Learning, RL)大致是介于监督学习与非监督学习之间，虽然没有一个监督者能够告诉强化学习的学习器正确的答案，但能够对学习器的选择给予反馈，David Silver（main programmer on the Go team at DeepMind Google）在[UCL的课程][2]中对RL有很好的介绍。\n\n强化学习是基于奖励假设(Reward Hypothesis)的一种学习过程：\n\n> **Reward Hypothesis:** All goals can be described by the maximisation of expected cumulative reward\n\n在强化学习中，通常会有一个预设的目标，如在试图在围棋对弈中取胜，围棋中的每一步行子并不会有绝对的正误之分，但却会对最终能否取胜产生影响，我们可以分析每一步对于最终获胜的贡献程度而选择最佳的落子方式。\n\n与其他的学习方式相比，RL具有如下特征：\n\n1. 不存在监督者，只有奖励信号\n2. 反馈会延迟，而非即时对应，一个动作的反馈信号可能会很久后到来\n3. 时间很重要，RL是一个动态的过程，这一过程经常被马尔科夫决策过程所描述，动作-状态对是序列发生的，并不能通过i.i.d数据进行学习；结合第二点，动作的反馈可能有长时与短时之分，短时获益的选择可能会导致一段时间后的损失，眼下并不能获益的行为可能最有助于达到最终的目标。\n4. 如上所述，学习器所获得的信号并非是固定的，而是由其自身的行为决定。某一时刻下学习器的选择会决定之后的状态。\n\n我们所能够遇到的场景通常也都与RL所假设的一致，总体上看是在探索未知领域与遵从已有知识间找到平衡。这种目标导向的学习方式相较于监督学习与非监督学习也更加灵活，也被认为是人类最为主要的学习方式之一。\n\n## 3.2 主动学习器与被动学习器\n\n学习器可以根据与获取训练数据的方式分为主动学习器和被动学习器。\n\n主动学习器会在学习时通过提问或实验的方式与环境交互，如一个学习器在学习标记垃圾邮件时，会主动挑选出一些邮件要求用户标记，从而提高对“垃圾邮件的理解”。\n\n被动学习器则完全依据事先提供的信息进行学习。\n\n## 3.3 在线学习与批量学习\n\n批量学习指一次性处理大量的数据后获取技能；在线学习指在新数据到来后，立即考虑新数据的影响并对学习结果进行更新，一般而言，这样的学习结果更加有效。而从人类的学习与分析表现上看，应当是具有实现类似于在线学习的能力。\n\n# 4. 学习的方法\n\n关于学习理论，有许多不同的流派持有不同的看法。这里就产生式分类和判别式分类，频率派与贝叶斯派的基本方法进行简单的介绍。\n\n## 4.1 生成的分类与识别的分类\n\n考虑如果已知观测对象是模式$\\boldsymbol{x}$，如这是急性昏迷病人的脑电模式，使得分类类别$y$（预后：转醒，PVS）在模式$\\boldsymbol{x}$下的条件概率$p(y\\mid\\boldsymbol{x})$最大的的类别$\\hat{y}$，就是这一病人相匹配的预后类别。\n$$\\hat{y}=\\mathrm{argmax\\; }p\\_y(y\\mid\\boldsymbol{x})$$\n这里”$\\mathrm{argmax}$ “是取最大值时的参数的意思。在模式识别里条件概率$p(y\\mid\\boldsymbol{x})$常被称作后验概率。应用训练集直接对后验概率$p(y\\mid\\boldsymbol{x})$进行学习的过程，称为判别式分析。\n\n此外，还可以把后验概率$p(y\\mid\\boldsymbol{x})$表示为$y$的函数\n$$\np(y\\mid\\boldsymbol{x})=\\frac{p(\\boldsymbol{x},y)}{p(\\boldsymbol{x})}\\propto{p(\\boldsymbol{x},y)}\n$$\n根据上式，我们可以发现模式$\\boldsymbol{x}$和类别$y$的联合概率$p(\\boldsymbol{x},y)$与后验概率$p(y|\\boldsymbol{x})$是成正比的。因此，我们可以通过使联合概率$p(\\boldsymbol{x},y)$达到最大值的方法，得到使后验概率$p(y\\mid\\boldsymbol{x})$达到最大值的类别$\\hat{y}$。\n$$\n\\hat{y}=\\mathrm{argmax\\;}p\\_y(\\boldsymbol{x},y)\n$$\n在模式识别力，联合概率$p(\\boldsymbol{x},y)$也被成为数据生成概率，通过预测数据生成概率$p(\\boldsymbol{x},y)$来进行模式识别的分类方法，称为生成的分类。\n\n一般而言，解决一个特殊问题所需的信息小于解决这个特殊问题所属的一般问题的信息。因为生成概率与后验概率的关系为：\n$$\np(y\\mid\\boldsymbol{x})=\\frac{p(\\boldsymbol{x},y)}{p(\\boldsymbol{x})}=\\frac{p(\\boldsymbol{x},y)}{\\Sigma\\_yp(\\boldsymbol{x},y)}\n$$\n在已知生成概率$p(\\boldsymbol{x},y)$的情况下可以很容易地得到后验概率$p(y\\mid\\boldsymbol{x})$，而已知后验概率$p(y\\mid\\boldsymbol{x})$的情况下却难以得到生成概率$p(\\boldsymbol{x},y)$。\n\n在具有一定先验知识的情况下，可以事先得到生成概率$p(\\boldsymbol{x},y)$，在先验概率的约束下，相当于具有了归纳偏置的能力，能对推断结果有更好的限制，相对而言生成的分类是更好的方式；而在没有先验知识，不知道生成概率$p(\\boldsymbol{x},y)$的情况下，则仅能够通过后验概率$p(y\\mid\\boldsymbol{x})$使用识别的分类进行推断了。\n\n## 4.2 统计概率和朴素贝叶斯\n\n统计概率的学习方法将对某一模式的描述看作是学习的结果；学习的目的是通过训练集如$\\mathcal{D}$，得到对包含参数$\\boldsymbol{\\theta}$的模式$q(\\boldsymbol{x},y;\\boldsymbol{\\theta})$的精确描述。\n$$\n\\max\\_\\theta\\prod\\_{i=1}^{y}q(\\boldsymbol{x}\\_i,y\\_i;\\boldsymbol{\\theta})\n$$\n朴素贝叶斯方法则将从模式$\\boldsymbol{\\theta}$产生的角度出发，将其看作是概率变量；依据贝叶斯定理，就可以通过求得的后验概率$p(\\boldsymbol{\\theta}\\mid\\mathcal{D})$估计先验概率$p(\\boldsymbol{\\theta})$。\n$$\np(\\boldsymbol{\\theta}\\mid\\mathcal{D})=\\frac{p(\\mathcal{D}\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathcal{D})}=\\frac{\\prod\\_{i=1}^nq(\\boldsymbol{x}\\_i,y\\_i\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{\\int\\prod\\_{i=1}^nq(\\boldsymbol{x}\\_i,y\\_i\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\mathrm{d}\\boldsymbol{\\theta}}\n$$\n\n# 5. 学习的理论框架\n\n## 5.1 学习器的输入\n\n### 领域集(domain set)\n\n领域集即我们希望学习的对象的总体，可以是任意的集合$\\mathcal{X}$。领域集中的元素通常可以用描述了其特征的向量表示，从心理学研究问题的角度出发，这些特征就是有关研究对象的因素，每个因素又存在不同的水平。如对于木瓜而言，其具有的特征有颜色，软硬程度等，对于颜色特征而言又有绿色，黄色等水平。在围棋对弈中，可以落子的位置构成了当前状态下的领域集。\n\n领域集中的元素也叫做实例，因此$\\mathcal{X}$也被叫做实例空间。\n\n### 标签集(label set)\n\n标签是指我们希望通过学习得到的有关实例的信息，标签集$\\mathcal{Y}$即是所有标签的集合。就木瓜的例子而言，我们会希望通过对于木瓜特征的学习，辨别木瓜好不好吃，因此给予木瓜的标签可以是好吃（为方便起见，用$1$表示）和不好吃（用$0$表示），因此对于木瓜的领域集$\\mathcal{X}$而言，其标签集$\\mathcal{Y}$为$\\{1,0\\}$。\n\n### 训练数据(training data)\n\n为了使得学习器学习到有关领域集的知识，通常我们都需要获得该领域集中的一些样本供学习器学习。训练数据至少应包含领域集内实例的部分特征，在监督学习情况下，训练数据还应包含实例所对应的标签，即训练数据以$\\mathcal{X}\\times\\mathcal{Y}$的形式成对出现，训练数据可以表示为训练集$S=\\{(X\\_1,Y\\_1),\\cdots,(x\\_m,y\\_m)\\}$这样一个有限的序列。\n\n## 5.2 学习器的输出\n\n学习器学习的目的是从训练数据中学会适用于训练数据所属领域集的知识，即可以给未加标签的实例加上预测的标签，这样的过程可以用一个描述从$\\mathcal{X}$到$\\mathcal{Y}$映射的函数表示，$h:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，这样的映射被称为预测规则(prediction rule)，这样的函数也可以被称为预测器(predictor)，假设(hypothesis)或分类器(classifier)。\n\n通常我们用$A(S)$来表示学习算法$A$在给定的训练序列$S$下返回的假设。\n\n## 5.3 获取训练数据\n\n从领域集中抽取合适的样本实际上是个很复杂的问题，通常我们会希望抽取的样本能与领域集独立同分布(i.i.d)，即训练数据$\\mathcal{S}$亦从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中抽取，服从$\\mathcal{D}$的分布。\n\n我们假设，对领域集而言，实例特征与标签间存在正确的映射关系$f:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，对于任意的$i$均有$y\\_i=f(x\\_i)$。学习器通过算法$A$学习训练数据，以正确的映射关系$f(x)$为目标，寻找$\\mathcal{X}$与$\\mathcal{Y}$间的映射关系，为训练数据赋予正确的标签，最终产生学习到的映射关系$h(x)$。\n\n## 5.4 评估学习结果\n\n分类器的误差被定义为，从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中随机抽取一个样本$x$，使得$h(x)\\boldsymbol\\ne f(x)$的概率。\n\n$h:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$的错误率被定义为$\\newcommand{\\eqdef}{\\overset{\\mathrm{def}}{=}}$ \n\n$$\nL_{\\mathcal{D},f}\\eqdef\\mathbb{P}\\_{x\\sim\\mathcal{D}}[h(x)\\ne f(x)]\\eqdef\\mathcal{D}(\\{x:h(x)\\neq f(x)\\})\n$$\n$L\\_{\\mathcal{D},f}(h)$也称为泛化误差，真实误差，或损失函数(cost function)，其下标$(\\mathcal{D},f)$表明误差的测量是基于概率分布$\\mathcal{D}$和正确标记函数$f(x)$进行的。在接下来对于PAC可学习性的讨论中，还会讨论这种损失的其他的可能形式。\n\n# 6. 经验风险最小化\n\n## 6.1 经验风险\n\n学习的过程是从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中获取训练集$S$，我们希望学习器能通过某种算法得到一个预测器$h\\_S:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，使得关于分布$\\mathcal{D}$和正确标记函数$f(x)$的预测误差最小化。\n\n由于我们并不知道分布$\\mathcal{D}$和正确标记函数$f(x)$究竟是什么样的，因此我们需要借助学习器在训练样本中的误差来对犯错的风险进行估计：\n$$\nL\\_S(h)\\eqdef\\frac{\\vert\\{i\\in[m]:h(x\\_i)\\ne y\\_i\\}\\vert}{m}\n$$\n其中，$[m]=\\{1,\\cdots,m\\}$\n\n这种从训练数据中得到的对风险的估计被称为经验风险或经验误差。由于我们假设训练集能够代表领域集的特征，因此，这种从训练集估计$\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$映射并以经验风险代表估计的真实风险的做法是可以接受的。这种寻求拥有最小经验风险的过程称为经验风险最小化(Empirical Risk Minization，ERM)。\n\n## 6.2 过拟合\n\n由于训练数据具有有限的数据量，如果不考虑寻找到的规则$h(x)$的复杂度，一定能够找到一种方法，将训练集中$\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$的关系拟合得很好。一个极端的例子是，如果学习过程变成了单纯的记忆，那么$L\\_\\mathcal{D}(h\\_S)$一定会等于$0$。\n\n通常过拟合发生时会导致，对训练数据拟合得非常好，但在训练数据之外表现得非常糟糕。\n\n# 7. 考虑归纳偏置的经验风险最小化\n\n## 7.1 归纳偏置的影响\n\n过拟合产生的一个重要原因在于，学习器对于真实的映射关系$f(x)$没有任何先验的概念，即不知道$f(x)$应该有的样子，$f(x)$本身应该有的性质无法对本应与$f(x)$一致的$h(x)$产生限制，$h(x)$为了适应数据而可以不考虑复杂度变得非常奇怪。\n\n又或者说，如果仅仅知道晚饭吃饱了而去推测晚上吃了什么，那么无论是中餐、西餐、日料、压缩饼干甚至是某些奇奇怪怪的东西都可以满足这一要求，但如果事先知道这是个中国人，而且是南方人，那么猜测的范围就可以大大缩小了。\n\n因此，在学习器接触训练集开始学习之前，就可以通过基于所学学习问题的先验知识，将基于训练集$S$的可能的选择器$h(x)$的范围限制于一个集合$\\mathcal{H}$中，这样的一个集合被称为假设类。在ERM准则下，假设类利用ERM规则进一步选择出合适的预测器$h\\boldsymbol\\in\\mathcal{H}$，这一过程可用如下形式表示\n$$\nERM\\_\\mathcal{H}(S)\\in\\mathrm{argmax\\;}\\_{h\\in\\mathcal{H}}L\\_S(h)\n$$\n接下来将会证明，为什么基于此假设类的$\\mathrm{ERM}_\\mathcal{H}$能够保证不过拟合，以及选择哪种假设类$\\mathrm{ERM}\\_\\mathcal{H}$不会导致过拟合。\n\n## 7.2 真实风险的来源\n\n### 置信参数(confidence parameter)\n\n由于学习器仅能通过训练集$S$学习在$\\mathcal{D}$上分布的领域集$\\mathcal{X}$，因此$S$与$\\mathcal{X}$分布的差异会导致真实误差$L\\_{\\mathcal{H},f}$的增加。为保证学习器所学习到的$h(x)$的准确性，需要保证$S$中的训练样本满足独立同分布(i.i.d.)假设，都是从$\\mathcal{D}$中独立同分布地抽取的，记为$S\\boldsymbol\\sim\\mathcal{D}^m$，其中$m$为$S$的势，即为在分布$\\mathcal{D}$下，独立同分布采样$S\\boldsymbol=z\\_1,\\cdots,z\\_m$。\n\n然而实际上，我们难以保证抽样的独立同分布，即使一学期仅有三次点名，一学期也仅skip三次课，也是有可能全部赶上点名的: )\n\n因此，我们将采样到不能代表领域集$\\mathcal{X}$的训练集$S$的概率表示为$\\delta$，并将$1-\\delta$称为该预测的置信参数(confidence parameter)\n\n### 精度参数(accuracy parameter)\n\n同样的，我们也无法保证$h(x)$所给出的预测结果的正确性，此时，我们使用精度参数(accuracy parameter)$\\varepsilon$来评价$h(x)$预测的质量，如果$L\\_{\\mathcal{D},f}>\\varepsilon$，那么$h(x)$就是一种失败的预测；如果$L\\_{\\mathcal{D},f}{\\leq}\\varepsilon$，那么就认为$h(x)$给出了一个误差可以接受的预测。\n\n## 7.3 学习的可靠性\n\n对于$S\\sim\\mathcal{D}^m$的训练集来说，设$\\mathcal{H}\\_B$是失败的假设集合，那么有：\n$$\n\\mathcal{H}\\_B=\\{h\\in\\mathcal{H}:L\\_{\\mathcal{D},f}(h)\\>\\varepsilon\\}\n$$\n设集合$M$是一个误导集，\n$$\nM=\\{S\\mid\\_x \\exists \\:h\\in\\mathcal{H}\\_B,L\\_S(h)=0\\}\n$$\n\n即$M$中的假设对于领域集来说是失败的预测，但对于训练集来说，则是符合精度参数要求的。即$M$描述了过拟合的$h(x)$。\n\n[1]:\thttp://utmemoryclub.com/wp-content/uploads/2013/07/brunel2016.pdf\n[2]:\thttp://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html","published":1,"updated":"2019-10-19T00:51:03.027Z","_id":"ckl5145an0005ysm6hswz8t7e","comments":1,"layout":"post","photos":[],"link":"","content":"<p>学习的基础是可学习性。可学习性包括三个方面：是否是可计算的；计算的复杂度如何；算法如何实现。在机器学习领域有一整套严格的形式化语言来描述这些问题。作为机器学习理论系列的第一篇，这里主要梳理了学习的概念、学习的分类、学习方法的大致框架等内容。</p>\n<a id=\"more\"></a>\n\n<h1 id=\"1-什么是学习\"><a href=\"#1-什么是学习\" class=\"headerlink\" title=\"1. 什么是学习\"></a>1. 什么是学习</h1><p>学习一词是我们再熟悉不过的了，从小时候总是被提及的好好学习，天天向上，到政治课本上的学习型社会，终身学习；从MOOC，远程教育等愈发多样的学习形式，再到引广泛争论的学习无用论，学习的概念充斥于我们的生活。</p>\n<p>学习也同样是心理学(Psychology)，认知神经科学(Cognitive Neuroscience, CN)，计算机科学(Computer Science, CS)等领域所关心的课题。个体的学习能力发展，学习的方法技巧，学习的分子机制，机器学习的原理与应用等等都是当今教育、科学领域的热门话题。</p>\n<p>学习之所以如此为家长们所汲汲，为大众所乐道，大概是因为学习总是与智慧的评价，解决问题的能力，竞争中所能取得的优势等相关联，看起来大概是有助于提升我们的适应与生存能力。</p>\n<p>我们对于学习是如此熟悉，学习对于我们而言却又如此陌生，我们对于学习的生理机制也还仅略知皮毛，对于machine learning的实现也还抱有一些疑问，甚至于我们没有一个统一而确切的概念去描述它。在Psychology和NS中，学习与记忆是一对经常被一起提到的概念，对于学习与记忆间的关系也常有着如下的定义：</p>\n<p>学习(Learning)是获取新信息的过程，学习的结果就是记忆(Memory)。学习与记忆可以假设为如下三个主要阶段：</p>\n<ol>\n<li>编码(encoding): 对输入的信息进行处理与储存，编码又可以分为获取(acquision)和巩固(consolidation)这两个过程。获取指的是对感觉通路和感觉分析阶段的输入信息进行登记；巩固是生成一个虽时间推移而增强的表征。</li>\n<li>存储(storage): 是获取和巩固的结果，代表了信息的长久记录。</li>\n<li>提取(retrieval): 是通过利用所存储的信息创建意识表征或执行习得的行为，如自动化动作。</li>\n</ol>\n<p>这似乎很符合我们的经验：我们将经验与知识保存为记忆，在新的情景下调用记忆处理新的问题，比如当学习过how to make a good cup of tea的protocol后，再次需要make tea的时候，按照记忆中的步骤一步步完成就可以了；我们将所确认过的骚扰电话存入一个数据库，当遇到一个新号码时，只需将这个号码和数据库中的号码一一比对，即可判断是不是骚扰电话。</p>\n<p>虽然这种基于记忆的学习方式看起来很有用，但有一些问题我们却不能忽视：当我们认识一个新朋友时，我们会记住他的面孔，口音，衣着等等；再次见面时，也许他的衣着变了，发型变了，虽然他和我们记忆中的表征有些不一样了，但我们还是会把他当作认识的那个人；即使说话习惯从广州白话变成了上海闲话，或许我们还是能够认出他来；但如果是从一个文艺青年变成了中二少年。。那个，不好意思，请问您是那位。。</p>\n<p>这提示我们：需要有多大的不同我们才会判断新出现的目标与我们记忆中的原型是不一致的呢？换言之，我们是通过什么标准来进行分类判别的呢？再把问题拓展一点，学习貌似不单单是与记忆做比较，成功的学习应当能够从个例出发进行泛化，在一定范围内对于目标的变异具有适应性，甚至在具有相似特征的对象间进行迁移：如果事先知道青的橘子，苹果，杏都很酸，那么在第一次遇见青的葡萄时，应当也可以假设，青的葡萄也是酸的；如果学习不具有这种能力的话，那么假如男朋友换了个发型就不认识了，那该是个多么悲伤的故事: )</p>\n<p>更何况，记忆本身的维持也是具有不同程度的区别，记忆的熟悉性与再认在功能与机制上可能也是分离的，有些事情我们明明已经记不大清了，但仍会影响到我们对于新事物的决策，如何解释这一现象，它们对于学习分别有何影响，也是需要我们去思考的。从学习的角度出发，或许记忆本就是学习的一种副产品，记忆的价值，进化中的动力，也几乎都是通过学习来体现的。进一步从学习机制的角度考虑，记忆也许仅仅是feedforward network学习过程中训练出的firing sequence的一种体现，<a href=\"http://utmemoryclub.com/wp-content/uploads/2013/07/brunel2016.pdf\">Brunel</a>的一些研究就为这一看法提供了支持。还有更多的一些可能，这里暂不展开讨论。</p>\n<h1 id=\"2-归纳偏置与credit-assignment-problem\"><a href=\"#2-归纳偏置与credit-assignment-problem\" class=\"headerlink\" title=\"2. 归纳偏置与credit assignment problem\"></a>2. 归纳偏置与credit assignment problem</h1><p>对于学习，一个概括性，不够全面，非操作性的定义是，学习是一个将经验转化为专业技能或知识的过程。操作性的定义大概在学习的一般模型中可以看到，这里我们先来讨论两个例子。</p>\n<p>第一个例子是B. F. Skinner著名的实验，迷信的鸽子。Skinner在鸽笼中放置了定时投喂装置，当食物第一次送达时，鸽子们处于不同的活动状态，如转动头部，转圈等。鸽子们误以为食物的到来与自己的动作有关，于是食物的到来强化了这种动作，为了得到更多的食物，鸽子们不断重复第一次食物送来时的动作状态。</p>\n<p>第二个例子是老鼠的怯饵效应。Garcia &amp; Koelling (1996)发现，当在大鼠进食后给予电击，大鼠并没有将电击与进食联系起来，即使将进食后的电击重复多次，大鼠仍然倾向于继续进食。当然，这可能仅仅因为大鼠觉得电就电好了，反正我要吃饱。。于是进一步地，研究者在大鼠的食物中混杂药物，导致大鼠食用食物后反胃呕吐，大鼠会避免再次进食；而如果在大鼠进食有毒食物的同时给予声音刺激，大鼠也并没有表现出如再次面对有毒食物时的害怕趋避表现。这似乎表明，大鼠具有某种“先验知识”，能够理解进食与电击，声音与反胃间并不存在直接的因果关系。</p>\n<p>鸽子与大鼠间行为差异的关键在于，可能存在的先验知识的引入使得学习机制出现了偏差，鸽子无法区分诸多线索中哪一个才是真正与反馈的出现存在因果关系，从而错误地将某一线索与反馈联系起来了；大鼠拥有关于食物、点击、不良反应、声音等的先验知识，在学习的过程中，这些先验知识使得学习的方向发生了偏差，从而获取有先验知识约束的更为准确的知识，这种现象被称做归纳偏置。</p>\n<p>在之后对于偏差和复杂度的讨论中我们可以证明，归纳偏置对于一个成功的算法而言是必不可少的部分。先验假设越强，就越容易从样本中进行可靠的学习；然而同时，过强的先验假设也会牺牲学习的灵活性。</p>\n<p>与鸽子与大鼠的经历类似，在生活中，我们常常会同时接受很多种刺激，如何将真正有价值的线索-反馈对分离出来，并针对某一特定反馈，为不同的线索的分配价值，这样的credit assignment problem是学习过程不可回避的问题。不幸的是，很多时候我们也并没有比鸽子做得更好，关于credit assignment problem更多的细节或许会在一个新的专题来进行讨论。</p>\n<h1 id=\"3-学习的种类\"><a href=\"#3-学习的种类\" class=\"headerlink\" title=\"3. 学习的种类\"></a>3. 学习的种类</h1><p>根据不同的分类标准，学习有多种分类方法。就学习理论的研究而言，可能会更多地倾向于从学习与环境的关系，学习的主动性，学习的形式这三个角度来进行分类。</p>\n<h2 id=\"3-1-学习与环境的关系\"><a href=\"#3-1-学习与环境的关系\" class=\"headerlink\" title=\"3.1 学习与环境的关系\"></a>3.1 学习与环境的关系</h2><p>如果把学习过程抽象为“利用经验获取技能的过程”，经验即是学习的对象，通过从经验中获取“技能”，解决新的问题即是学习的结果。在不同的情景下我们能够利用的经验也是不同的。根据学习时能够从经验中获取的信息，可以将学习分为监督学习，非监督学习，和强化学习三类。</p>\n<h3 id=\"监督学习-Supervised-Learning\"><a href=\"#监督学习-Supervised-Learning\" class=\"headerlink\" title=\"监督学习(Supervised Learning)\"></a>监督学习(Supervised Learning)</h3><p>监督学习(Unsupervised Learning)顾名思义指的是在学习的过程中存在“监督者”对学习的结果，即提供正确的输入输出信息供学习器（脑或机器）学习。如对于急性昏迷病人的预后判断，我们希望可以通过病人的脑电数据检测预测病人的预后（能够转醒或进入持续性植物状态，PVS）。那么我们就可以事先获得已知预后的病人的资料，并将脑电数据与预后状况配对，交给学习器进行学习。</p>\n<p>在这种情况下经验中包含了有关学习对象的完整的信息，通过已知正确的输入与输出对，学习所提供样本的特征，从而对与训练数据相似的新的数据进行分类，或检测其中的异常值。在上面的例子中，如果学习器学会了脑电数据与预后间的关系，那么就可以利用这种关系预测新的病人的预后了。</p>\n<h3 id=\"非监督学习-Unsupervised-Learning\"><a href=\"#非监督学习-Unsupervised-Learning\" class=\"headerlink\" title=\"非监督学习(Unsupervised Learning)\"></a>非监督学习(Unsupervised Learning)</h3><p>在非监督学习(Unsupervised Learning)中，经验没有提供学习对象额外的信息，用于学习的对象即训练数据与学习器在学习后需要接触到的实际数据间没有区别。同样以急性昏迷病人的预后判断为例，我们只是单纯地提供给学习器病人的脑电数据，学习器需要自己在这些数据中获取信息，发现规律，通常的结果是，学习器可以从这些数据中提取出概括的信息，即降维，如找出脑电信号中的主要成分，或是进行聚类，将相似的数据归为一类，如在理想情况下，学习器可能会自然地将病人分为两类，恰好对应预后转醒和PVS。</p>\n<h3 id=\"强化学习-Reinforcement-learning-RL\"><a href=\"#强化学习-Reinforcement-learning-RL\" class=\"headerlink\" title=\"强化学习(Reinforcement learning, RL)\"></a>强化学习(Reinforcement learning, RL)</h3><p>强化学习(Reinforcement Learning, RL)大致是介于监督学习与非监督学习之间，虽然没有一个监督者能够告诉强化学习的学习器正确的答案，但能够对学习器的选择给予反馈，David Silver（main programmer on the Go team at DeepMind Google）在<a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\">UCL的课程</a>中对RL有很好的介绍。</p>\n<p>强化学习是基于奖励假设(Reward Hypothesis)的一种学习过程：</p>\n<blockquote>\n<p><strong>Reward Hypothesis:</strong> All goals can be described by the maximisation of expected cumulative reward</p>\n</blockquote>\n<p>在强化学习中，通常会有一个预设的目标，如在试图在围棋对弈中取胜，围棋中的每一步行子并不会有绝对的正误之分，但却会对最终能否取胜产生影响，我们可以分析每一步对于最终获胜的贡献程度而选择最佳的落子方式。</p>\n<p>与其他的学习方式相比，RL具有如下特征：</p>\n<ol>\n<li>不存在监督者，只有奖励信号</li>\n<li>反馈会延迟，而非即时对应，一个动作的反馈信号可能会很久后到来</li>\n<li>时间很重要，RL是一个动态的过程，这一过程经常被马尔科夫决策过程所描述，动作-状态对是序列发生的，并不能通过i.i.d数据进行学习；结合第二点，动作的反馈可能有长时与短时之分，短时获益的选择可能会导致一段时间后的损失，眼下并不能获益的行为可能最有助于达到最终的目标。</li>\n<li>如上所述，学习器所获得的信号并非是固定的，而是由其自身的行为决定。某一时刻下学习器的选择会决定之后的状态。</li>\n</ol>\n<p>我们所能够遇到的场景通常也都与RL所假设的一致，总体上看是在探索未知领域与遵从已有知识间找到平衡。这种目标导向的学习方式相较于监督学习与非监督学习也更加灵活，也被认为是人类最为主要的学习方式之一。</p>\n<h2 id=\"3-2-主动学习器与被动学习器\"><a href=\"#3-2-主动学习器与被动学习器\" class=\"headerlink\" title=\"3.2 主动学习器与被动学习器\"></a>3.2 主动学习器与被动学习器</h2><p>学习器可以根据与获取训练数据的方式分为主动学习器和被动学习器。</p>\n<p>主动学习器会在学习时通过提问或实验的方式与环境交互，如一个学习器在学习标记垃圾邮件时，会主动挑选出一些邮件要求用户标记，从而提高对“垃圾邮件的理解”。</p>\n<p>被动学习器则完全依据事先提供的信息进行学习。</p>\n<h2 id=\"3-3-在线学习与批量学习\"><a href=\"#3-3-在线学习与批量学习\" class=\"headerlink\" title=\"3.3 在线学习与批量学习\"></a>3.3 在线学习与批量学习</h2><p>批量学习指一次性处理大量的数据后获取技能；在线学习指在新数据到来后，立即考虑新数据的影响并对学习结果进行更新，一般而言，这样的学习结果更加有效。而从人类的学习与分析表现上看，应当是具有实现类似于在线学习的能力。</p>\n<h1 id=\"4-学习的方法\"><a href=\"#4-学习的方法\" class=\"headerlink\" title=\"4. 学习的方法\"></a>4. 学习的方法</h1><p>关于学习理论，有许多不同的流派持有不同的看法。这里就产生式分类和判别式分类，频率派与贝叶斯派的基本方法进行简单的介绍。</p>\n<h2 id=\"4-1-生成的分类与识别的分类\"><a href=\"#4-1-生成的分类与识别的分类\" class=\"headerlink\" title=\"4.1 生成的分类与识别的分类\"></a>4.1 生成的分类与识别的分类</h2><p>考虑如果已知观测对象是模式$\\boldsymbol{x}$，如这是急性昏迷病人的脑电模式，使得分类类别$y$（预后：转醒，PVS）在模式$\\boldsymbol{x}$下的条件概率$p(y\\mid\\boldsymbol{x})$最大的的类别$\\hat{y}$，就是这一病人相匹配的预后类别。<br>$$\\hat{y}=\\mathrm{argmax; }p_y(y\\mid\\boldsymbol{x})$$<br>这里”$\\mathrm{argmax}$ “是取最大值时的参数的意思。在模式识别里条件概率$p(y\\mid\\boldsymbol{x})$常被称作后验概率。应用训练集直接对后验概率$p(y\\mid\\boldsymbol{x})$进行学习的过程，称为判别式分析。</p>\n<p>此外，还可以把后验概率$p(y\\mid\\boldsymbol{x})$表示为$y$的函数<br>$$<br>p(y\\mid\\boldsymbol{x})=\\frac{p(\\boldsymbol{x},y)}{p(\\boldsymbol{x})}\\propto{p(\\boldsymbol{x},y)}<br>$$<br>根据上式，我们可以发现模式$\\boldsymbol{x}$和类别$y$的联合概率$p(\\boldsymbol{x},y)$与后验概率$p(y|\\boldsymbol{x})$是成正比的。因此，我们可以通过使联合概率$p(\\boldsymbol{x},y)$达到最大值的方法，得到使后验概率$p(y\\mid\\boldsymbol{x})$达到最大值的类别$\\hat{y}$。<br>$$<br>\\hat{y}=\\mathrm{argmax;}p_y(\\boldsymbol{x},y)<br>$$<br>在模式识别力，联合概率$p(\\boldsymbol{x},y)$也被成为数据生成概率，通过预测数据生成概率$p(\\boldsymbol{x},y)$来进行模式识别的分类方法，称为生成的分类。</p>\n<p>一般而言，解决一个特殊问题所需的信息小于解决这个特殊问题所属的一般问题的信息。因为生成概率与后验概率的关系为：<br>$$<br>p(y\\mid\\boldsymbol{x})=\\frac{p(\\boldsymbol{x},y)}{p(\\boldsymbol{x})}=\\frac{p(\\boldsymbol{x},y)}{\\Sigma_yp(\\boldsymbol{x},y)}<br>$$<br>在已知生成概率$p(\\boldsymbol{x},y)$的情况下可以很容易地得到后验概率$p(y\\mid\\boldsymbol{x})$，而已知后验概率$p(y\\mid\\boldsymbol{x})$的情况下却难以得到生成概率$p(\\boldsymbol{x},y)$。</p>\n<p>在具有一定先验知识的情况下，可以事先得到生成概率$p(\\boldsymbol{x},y)$，在先验概率的约束下，相当于具有了归纳偏置的能力，能对推断结果有更好的限制，相对而言生成的分类是更好的方式；而在没有先验知识，不知道生成概率$p(\\boldsymbol{x},y)$的情况下，则仅能够通过后验概率$p(y\\mid\\boldsymbol{x})$使用识别的分类进行推断了。</p>\n<h2 id=\"4-2-统计概率和朴素贝叶斯\"><a href=\"#4-2-统计概率和朴素贝叶斯\" class=\"headerlink\" title=\"4.2 统计概率和朴素贝叶斯\"></a>4.2 统计概率和朴素贝叶斯</h2><p>统计概率的学习方法将对某一模式的描述看作是学习的结果；学习的目的是通过训练集如$\\mathcal{D}$，得到对包含参数$\\boldsymbol{\\theta}$的模式$q(\\boldsymbol{x},y;\\boldsymbol{\\theta})$的精确描述。<br>$$<br>\\max_\\theta\\prod_{i=1}^{y}q(\\boldsymbol{x}_i,y_i;\\boldsymbol{\\theta})<br>$$<br>朴素贝叶斯方法则将从模式$\\boldsymbol{\\theta}$产生的角度出发，将其看作是概率变量；依据贝叶斯定理，就可以通过求得的后验概率$p(\\boldsymbol{\\theta}\\mid\\mathcal{D})$估计先验概率$p(\\boldsymbol{\\theta})$。<br>$$<br>p(\\boldsymbol{\\theta}\\mid\\mathcal{D})=\\frac{p(\\mathcal{D}\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathcal{D})}=\\frac{\\prod_{i=1}^nq(\\boldsymbol{x}_i,y_i\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{\\int\\prod_{i=1}^nq(\\boldsymbol{x}_i,y_i\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\mathrm{d}\\boldsymbol{\\theta}}<br>$$</p>\n<h1 id=\"5-学习的理论框架\"><a href=\"#5-学习的理论框架\" class=\"headerlink\" title=\"5. 学习的理论框架\"></a>5. 学习的理论框架</h1><h2 id=\"5-1-学习器的输入\"><a href=\"#5-1-学习器的输入\" class=\"headerlink\" title=\"5.1 学习器的输入\"></a>5.1 学习器的输入</h2><h3 id=\"领域集-domain-set\"><a href=\"#领域集-domain-set\" class=\"headerlink\" title=\"领域集(domain set)\"></a>领域集(domain set)</h3><p>领域集即我们希望学习的对象的总体，可以是任意的集合$\\mathcal{X}$。领域集中的元素通常可以用描述了其特征的向量表示，从心理学研究问题的角度出发，这些特征就是有关研究对象的因素，每个因素又存在不同的水平。如对于木瓜而言，其具有的特征有颜色，软硬程度等，对于颜色特征而言又有绿色，黄色等水平。在围棋对弈中，可以落子的位置构成了当前状态下的领域集。</p>\n<p>领域集中的元素也叫做实例，因此$\\mathcal{X}$也被叫做实例空间。</p>\n<h3 id=\"标签集-label-set\"><a href=\"#标签集-label-set\" class=\"headerlink\" title=\"标签集(label set)\"></a>标签集(label set)</h3><p>标签是指我们希望通过学习得到的有关实例的信息，标签集$\\mathcal{Y}$即是所有标签的集合。就木瓜的例子而言，我们会希望通过对于木瓜特征的学习，辨别木瓜好不好吃，因此给予木瓜的标签可以是好吃（为方便起见，用$1$表示）和不好吃（用$0$表示），因此对于木瓜的领域集$\\mathcal{X}$而言，其标签集$\\mathcal{Y}$为${1,0}$。</p>\n<h3 id=\"训练数据-training-data\"><a href=\"#训练数据-training-data\" class=\"headerlink\" title=\"训练数据(training data)\"></a>训练数据(training data)</h3><p>为了使得学习器学习到有关领域集的知识，通常我们都需要获得该领域集中的一些样本供学习器学习。训练数据至少应包含领域集内实例的部分特征，在监督学习情况下，训练数据还应包含实例所对应的标签，即训练数据以$\\mathcal{X}\\times\\mathcal{Y}$的形式成对出现，训练数据可以表示为训练集$S={(X_1,Y_1),\\cdots,(x_m,y_m)}$这样一个有限的序列。</p>\n<h2 id=\"5-2-学习器的输出\"><a href=\"#5-2-学习器的输出\" class=\"headerlink\" title=\"5.2 学习器的输出\"></a>5.2 学习器的输出</h2><p>学习器学习的目的是从训练数据中学会适用于训练数据所属领域集的知识，即可以给未加标签的实例加上预测的标签，这样的过程可以用一个描述从$\\mathcal{X}$到$\\mathcal{Y}$映射的函数表示，$h:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，这样的映射被称为预测规则(prediction rule)，这样的函数也可以被称为预测器(predictor)，假设(hypothesis)或分类器(classifier)。</p>\n<p>通常我们用$A(S)$来表示学习算法$A$在给定的训练序列$S$下返回的假设。</p>\n<h2 id=\"5-3-获取训练数据\"><a href=\"#5-3-获取训练数据\" class=\"headerlink\" title=\"5.3 获取训练数据\"></a>5.3 获取训练数据</h2><p>从领域集中抽取合适的样本实际上是个很复杂的问题，通常我们会希望抽取的样本能与领域集独立同分布(i.i.d)，即训练数据$\\mathcal{S}$亦从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中抽取，服从$\\mathcal{D}$的分布。</p>\n<p>我们假设，对领域集而言，实例特征与标签间存在正确的映射关系$f:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，对于任意的$i$均有$y_i=f(x_i)$。学习器通过算法$A$学习训练数据，以正确的映射关系$f(x)$为目标，寻找$\\mathcal{X}$与$\\mathcal{Y}$间的映射关系，为训练数据赋予正确的标签，最终产生学习到的映射关系$h(x)$。</p>\n<h2 id=\"5-4-评估学习结果\"><a href=\"#5-4-评估学习结果\" class=\"headerlink\" title=\"5.4 评估学习结果\"></a>5.4 评估学习结果</h2><p>分类器的误差被定义为，从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中随机抽取一个样本$x$，使得$h(x)\\boldsymbol\\ne f(x)$的概率。</p>\n<p>$h:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$的错误率被定义为$\\newcommand{\\eqdef}{\\overset{\\mathrm{def}}{=}}$ </p>\n<p>$$<br>L_{\\mathcal{D},f}\\eqdef\\mathbb{P}_{x\\sim\\mathcal{D}}[h(x)\\ne f(x)]\\eqdef\\mathcal{D}({x:h(x)\\neq f(x)})<br>$$<br>$L_{\\mathcal{D},f}(h)$也称为泛化误差，真实误差，或损失函数(cost function)，其下标$(\\mathcal{D},f)$表明误差的测量是基于概率分布$\\mathcal{D}$和正确标记函数$f(x)$进行的。在接下来对于PAC可学习性的讨论中，还会讨论这种损失的其他的可能形式。</p>\n<h1 id=\"6-经验风险最小化\"><a href=\"#6-经验风险最小化\" class=\"headerlink\" title=\"6. 经验风险最小化\"></a>6. 经验风险最小化</h1><h2 id=\"6-1-经验风险\"><a href=\"#6-1-经验风险\" class=\"headerlink\" title=\"6.1 经验风险\"></a>6.1 经验风险</h2><p>学习的过程是从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中获取训练集$S$，我们希望学习器能通过某种算法得到一个预测器$h_S:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，使得关于分布$\\mathcal{D}$和正确标记函数$f(x)$的预测误差最小化。</p>\n<p>由于我们并不知道分布$\\mathcal{D}$和正确标记函数$f(x)$究竟是什么样的，因此我们需要借助学习器在训练样本中的误差来对犯错的风险进行估计：<br>$$<br>L_S(h)\\eqdef\\frac{\\vert{i\\in[m]:h(x_i)\\ne y_i}\\vert}{m}<br>$$<br>其中，$[m]={1,\\cdots,m}$</p>\n<p>这种从训练数据中得到的对风险的估计被称为经验风险或经验误差。由于我们假设训练集能够代表领域集的特征，因此，这种从训练集估计$\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$映射并以经验风险代表估计的真实风险的做法是可以接受的。这种寻求拥有最小经验风险的过程称为经验风险最小化(Empirical Risk Minization，ERM)。</p>\n<h2 id=\"6-2-过拟合\"><a href=\"#6-2-过拟合\" class=\"headerlink\" title=\"6.2 过拟合\"></a>6.2 过拟合</h2><p>由于训练数据具有有限的数据量，如果不考虑寻找到的规则$h(x)$的复杂度，一定能够找到一种方法，将训练集中$\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$的关系拟合得很好。一个极端的例子是，如果学习过程变成了单纯的记忆，那么$L_\\mathcal{D}(h_S)$一定会等于$0$。</p>\n<p>通常过拟合发生时会导致，对训练数据拟合得非常好，但在训练数据之外表现得非常糟糕。</p>\n<h1 id=\"7-考虑归纳偏置的经验风险最小化\"><a href=\"#7-考虑归纳偏置的经验风险最小化\" class=\"headerlink\" title=\"7. 考虑归纳偏置的经验风险最小化\"></a>7. 考虑归纳偏置的经验风险最小化</h1><h2 id=\"7-1-归纳偏置的影响\"><a href=\"#7-1-归纳偏置的影响\" class=\"headerlink\" title=\"7.1 归纳偏置的影响\"></a>7.1 归纳偏置的影响</h2><p>过拟合产生的一个重要原因在于，学习器对于真实的映射关系$f(x)$没有任何先验的概念，即不知道$f(x)$应该有的样子，$f(x)$本身应该有的性质无法对本应与$f(x)$一致的$h(x)$产生限制，$h(x)$为了适应数据而可以不考虑复杂度变得非常奇怪。</p>\n<p>又或者说，如果仅仅知道晚饭吃饱了而去推测晚上吃了什么，那么无论是中餐、西餐、日料、压缩饼干甚至是某些奇奇怪怪的东西都可以满足这一要求，但如果事先知道这是个中国人，而且是南方人，那么猜测的范围就可以大大缩小了。</p>\n<p>因此，在学习器接触训练集开始学习之前，就可以通过基于所学学习问题的先验知识，将基于训练集$S$的可能的选择器$h(x)$的范围限制于一个集合$\\mathcal{H}$中，这样的一个集合被称为假设类。在ERM准则下，假设类利用ERM规则进一步选择出合适的预测器$h\\boldsymbol\\in\\mathcal{H}$，这一过程可用如下形式表示<br>$$<br>ERM_\\mathcal{H}(S)\\in\\mathrm{argmax;}_{h\\in\\mathcal{H}}L_S(h)<br>$$<br>接下来将会证明，为什么基于此假设类的$\\mathrm{ERM}_\\mathcal{H}$能够保证不过拟合，以及选择哪种假设类$\\mathrm{ERM}_\\mathcal{H}$不会导致过拟合。</p>\n<h2 id=\"7-2-真实风险的来源\"><a href=\"#7-2-真实风险的来源\" class=\"headerlink\" title=\"7.2 真实风险的来源\"></a>7.2 真实风险的来源</h2><h3 id=\"置信参数-confidence-parameter\"><a href=\"#置信参数-confidence-parameter\" class=\"headerlink\" title=\"置信参数(confidence parameter)\"></a>置信参数(confidence parameter)</h3><p>由于学习器仅能通过训练集$S$学习在$\\mathcal{D}$上分布的领域集$\\mathcal{X}$，因此$S$与$\\mathcal{X}$分布的差异会导致真实误差$L_{\\mathcal{H},f}$的增加。为保证学习器所学习到的$h(x)$的准确性，需要保证$S$中的训练样本满足独立同分布(i.i.d.)假设，都是从$\\mathcal{D}$中独立同分布地抽取的，记为$S\\boldsymbol\\sim\\mathcal{D}^m$，其中$m$为$S$的势，即为在分布$\\mathcal{D}$下，独立同分布采样$S\\boldsymbol=z_1,\\cdots,z_m$。</p>\n<p>然而实际上，我们难以保证抽样的独立同分布，即使一学期仅有三次点名，一学期也仅skip三次课，也是有可能全部赶上点名的: )</p>\n<p>因此，我们将采样到不能代表领域集$\\mathcal{X}$的训练集$S$的概率表示为$\\delta$，并将$1-\\delta$称为该预测的置信参数(confidence parameter)</p>\n<h3 id=\"精度参数-accuracy-parameter\"><a href=\"#精度参数-accuracy-parameter\" class=\"headerlink\" title=\"精度参数(accuracy parameter)\"></a>精度参数(accuracy parameter)</h3><p>同样的，我们也无法保证$h(x)$所给出的预测结果的正确性，此时，我们使用精度参数(accuracy parameter)$\\varepsilon$来评价$h(x)$预测的质量，如果$L_{\\mathcal{D},f}&gt;\\varepsilon$，那么$h(x)$就是一种失败的预测；如果$L_{\\mathcal{D},f}{\\leq}\\varepsilon$，那么就认为$h(x)$给出了一个误差可以接受的预测。</p>\n<h2 id=\"7-3-学习的可靠性\"><a href=\"#7-3-学习的可靠性\" class=\"headerlink\" title=\"7.3 学习的可靠性\"></a>7.3 学习的可靠性</h2><p>对于$S\\sim\\mathcal{D}^m$的训练集来说，设$\\mathcal{H}_B$是失败的假设集合，那么有：<br>$$<br>\\mathcal{H}_B={h\\in\\mathcal{H}:L_{\\mathcal{D},f}(h)&gt;\\varepsilon}<br>$$<br>设集合$M$是一个误导集，<br>$$<br>M={S\\mid_x \\exists :h\\in\\mathcal{H}_B,L_S(h)=0}<br>$$</p>\n<p>即$M$中的假设对于领域集来说是失败的预测，但对于训练集来说，则是符合精度参数要求的。即$M$描述了过拟合的$h(x)$。</p>\n","site":{"data":{}},"excerpt":"<p>学习的基础是可学习性。可学习性包括三个方面：是否是可计算的；计算的复杂度如何；算法如何实现。在机器学习领域有一整套严格的形式化语言来描述这些问题。作为机器学习理论系列的第一篇，这里主要梳理了学习的概念、学习的分类、学习方法的大致框架等内容。</p>","more":"<h1 id=\"1-什么是学习\"><a href=\"#1-什么是学习\" class=\"headerlink\" title=\"1. 什么是学习\"></a>1. 什么是学习</h1><p>学习一词是我们再熟悉不过的了，从小时候总是被提及的好好学习，天天向上，到政治课本上的学习型社会，终身学习；从MOOC，远程教育等愈发多样的学习形式，再到引广泛争论的学习无用论，学习的概念充斥于我们的生活。</p>\n<p>学习也同样是心理学(Psychology)，认知神经科学(Cognitive Neuroscience, CN)，计算机科学(Computer Science, CS)等领域所关心的课题。个体的学习能力发展，学习的方法技巧，学习的分子机制，机器学习的原理与应用等等都是当今教育、科学领域的热门话题。</p>\n<p>学习之所以如此为家长们所汲汲，为大众所乐道，大概是因为学习总是与智慧的评价，解决问题的能力，竞争中所能取得的优势等相关联，看起来大概是有助于提升我们的适应与生存能力。</p>\n<p>我们对于学习是如此熟悉，学习对于我们而言却又如此陌生，我们对于学习的生理机制也还仅略知皮毛，对于machine learning的实现也还抱有一些疑问，甚至于我们没有一个统一而确切的概念去描述它。在Psychology和NS中，学习与记忆是一对经常被一起提到的概念，对于学习与记忆间的关系也常有着如下的定义：</p>\n<p>学习(Learning)是获取新信息的过程，学习的结果就是记忆(Memory)。学习与记忆可以假设为如下三个主要阶段：</p>\n<ol>\n<li>编码(encoding): 对输入的信息进行处理与储存，编码又可以分为获取(acquision)和巩固(consolidation)这两个过程。获取指的是对感觉通路和感觉分析阶段的输入信息进行登记；巩固是生成一个虽时间推移而增强的表征。</li>\n<li>存储(storage): 是获取和巩固的结果，代表了信息的长久记录。</li>\n<li>提取(retrieval): 是通过利用所存储的信息创建意识表征或执行习得的行为，如自动化动作。</li>\n</ol>\n<p>这似乎很符合我们的经验：我们将经验与知识保存为记忆，在新的情景下调用记忆处理新的问题，比如当学习过how to make a good cup of tea的protocol后，再次需要make tea的时候，按照记忆中的步骤一步步完成就可以了；我们将所确认过的骚扰电话存入一个数据库，当遇到一个新号码时，只需将这个号码和数据库中的号码一一比对，即可判断是不是骚扰电话。</p>\n<p>虽然这种基于记忆的学习方式看起来很有用，但有一些问题我们却不能忽视：当我们认识一个新朋友时，我们会记住他的面孔，口音，衣着等等；再次见面时，也许他的衣着变了，发型变了，虽然他和我们记忆中的表征有些不一样了，但我们还是会把他当作认识的那个人；即使说话习惯从广州白话变成了上海闲话，或许我们还是能够认出他来；但如果是从一个文艺青年变成了中二少年。。那个，不好意思，请问您是那位。。</p>\n<p>这提示我们：需要有多大的不同我们才会判断新出现的目标与我们记忆中的原型是不一致的呢？换言之，我们是通过什么标准来进行分类判别的呢？再把问题拓展一点，学习貌似不单单是与记忆做比较，成功的学习应当能够从个例出发进行泛化，在一定范围内对于目标的变异具有适应性，甚至在具有相似特征的对象间进行迁移：如果事先知道青的橘子，苹果，杏都很酸，那么在第一次遇见青的葡萄时，应当也可以假设，青的葡萄也是酸的；如果学习不具有这种能力的话，那么假如男朋友换了个发型就不认识了，那该是个多么悲伤的故事: )</p>\n<p>更何况，记忆本身的维持也是具有不同程度的区别，记忆的熟悉性与再认在功能与机制上可能也是分离的，有些事情我们明明已经记不大清了，但仍会影响到我们对于新事物的决策，如何解释这一现象，它们对于学习分别有何影响，也是需要我们去思考的。从学习的角度出发，或许记忆本就是学习的一种副产品，记忆的价值，进化中的动力，也几乎都是通过学习来体现的。进一步从学习机制的角度考虑，记忆也许仅仅是feedforward network学习过程中训练出的firing sequence的一种体现，<a href=\"http://utmemoryclub.com/wp-content/uploads/2013/07/brunel2016.pdf\">Brunel</a>的一些研究就为这一看法提供了支持。还有更多的一些可能，这里暂不展开讨论。</p>\n<h1 id=\"2-归纳偏置与credit-assignment-problem\"><a href=\"#2-归纳偏置与credit-assignment-problem\" class=\"headerlink\" title=\"2. 归纳偏置与credit assignment problem\"></a>2. 归纳偏置与credit assignment problem</h1><p>对于学习，一个概括性，不够全面，非操作性的定义是，学习是一个将经验转化为专业技能或知识的过程。操作性的定义大概在学习的一般模型中可以看到，这里我们先来讨论两个例子。</p>\n<p>第一个例子是B. F. Skinner著名的实验，迷信的鸽子。Skinner在鸽笼中放置了定时投喂装置，当食物第一次送达时，鸽子们处于不同的活动状态，如转动头部，转圈等。鸽子们误以为食物的到来与自己的动作有关，于是食物的到来强化了这种动作，为了得到更多的食物，鸽子们不断重复第一次食物送来时的动作状态。</p>\n<p>第二个例子是老鼠的怯饵效应。Garcia &amp; Koelling (1996)发现，当在大鼠进食后给予电击，大鼠并没有将电击与进食联系起来，即使将进食后的电击重复多次，大鼠仍然倾向于继续进食。当然，这可能仅仅因为大鼠觉得电就电好了，反正我要吃饱。。于是进一步地，研究者在大鼠的食物中混杂药物，导致大鼠食用食物后反胃呕吐，大鼠会避免再次进食；而如果在大鼠进食有毒食物的同时给予声音刺激，大鼠也并没有表现出如再次面对有毒食物时的害怕趋避表现。这似乎表明，大鼠具有某种“先验知识”，能够理解进食与电击，声音与反胃间并不存在直接的因果关系。</p>\n<p>鸽子与大鼠间行为差异的关键在于，可能存在的先验知识的引入使得学习机制出现了偏差，鸽子无法区分诸多线索中哪一个才是真正与反馈的出现存在因果关系，从而错误地将某一线索与反馈联系起来了；大鼠拥有关于食物、点击、不良反应、声音等的先验知识，在学习的过程中，这些先验知识使得学习的方向发生了偏差，从而获取有先验知识约束的更为准确的知识，这种现象被称做归纳偏置。</p>\n<p>在之后对于偏差和复杂度的讨论中我们可以证明，归纳偏置对于一个成功的算法而言是必不可少的部分。先验假设越强，就越容易从样本中进行可靠的学习；然而同时，过强的先验假设也会牺牲学习的灵活性。</p>\n<p>与鸽子与大鼠的经历类似，在生活中，我们常常会同时接受很多种刺激，如何将真正有价值的线索-反馈对分离出来，并针对某一特定反馈，为不同的线索的分配价值，这样的credit assignment problem是学习过程不可回避的问题。不幸的是，很多时候我们也并没有比鸽子做得更好，关于credit assignment problem更多的细节或许会在一个新的专题来进行讨论。</p>\n<h1 id=\"3-学习的种类\"><a href=\"#3-学习的种类\" class=\"headerlink\" title=\"3. 学习的种类\"></a>3. 学习的种类</h1><p>根据不同的分类标准，学习有多种分类方法。就学习理论的研究而言，可能会更多地倾向于从学习与环境的关系，学习的主动性，学习的形式这三个角度来进行分类。</p>\n<h2 id=\"3-1-学习与环境的关系\"><a href=\"#3-1-学习与环境的关系\" class=\"headerlink\" title=\"3.1 学习与环境的关系\"></a>3.1 学习与环境的关系</h2><p>如果把学习过程抽象为“利用经验获取技能的过程”，经验即是学习的对象，通过从经验中获取“技能”，解决新的问题即是学习的结果。在不同的情景下我们能够利用的经验也是不同的。根据学习时能够从经验中获取的信息，可以将学习分为监督学习，非监督学习，和强化学习三类。</p>\n<h3 id=\"监督学习-Supervised-Learning\"><a href=\"#监督学习-Supervised-Learning\" class=\"headerlink\" title=\"监督学习(Supervised Learning)\"></a>监督学习(Supervised Learning)</h3><p>监督学习(Unsupervised Learning)顾名思义指的是在学习的过程中存在“监督者”对学习的结果，即提供正确的输入输出信息供学习器（脑或机器）学习。如对于急性昏迷病人的预后判断，我们希望可以通过病人的脑电数据检测预测病人的预后（能够转醒或进入持续性植物状态，PVS）。那么我们就可以事先获得已知预后的病人的资料，并将脑电数据与预后状况配对，交给学习器进行学习。</p>\n<p>在这种情况下经验中包含了有关学习对象的完整的信息，通过已知正确的输入与输出对，学习所提供样本的特征，从而对与训练数据相似的新的数据进行分类，或检测其中的异常值。在上面的例子中，如果学习器学会了脑电数据与预后间的关系，那么就可以利用这种关系预测新的病人的预后了。</p>\n<h3 id=\"非监督学习-Unsupervised-Learning\"><a href=\"#非监督学习-Unsupervised-Learning\" class=\"headerlink\" title=\"非监督学习(Unsupervised Learning)\"></a>非监督学习(Unsupervised Learning)</h3><p>在非监督学习(Unsupervised Learning)中，经验没有提供学习对象额外的信息，用于学习的对象即训练数据与学习器在学习后需要接触到的实际数据间没有区别。同样以急性昏迷病人的预后判断为例，我们只是单纯地提供给学习器病人的脑电数据，学习器需要自己在这些数据中获取信息，发现规律，通常的结果是，学习器可以从这些数据中提取出概括的信息，即降维，如找出脑电信号中的主要成分，或是进行聚类，将相似的数据归为一类，如在理想情况下，学习器可能会自然地将病人分为两类，恰好对应预后转醒和PVS。</p>\n<h3 id=\"强化学习-Reinforcement-learning-RL\"><a href=\"#强化学习-Reinforcement-learning-RL\" class=\"headerlink\" title=\"强化学习(Reinforcement learning, RL)\"></a>强化学习(Reinforcement learning, RL)</h3><p>强化学习(Reinforcement Learning, RL)大致是介于监督学习与非监督学习之间，虽然没有一个监督者能够告诉强化学习的学习器正确的答案，但能够对学习器的选择给予反馈，David Silver（main programmer on the Go team at DeepMind Google）在<a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\">UCL的课程</a>中对RL有很好的介绍。</p>\n<p>强化学习是基于奖励假设(Reward Hypothesis)的一种学习过程：</p>\n<blockquote>\n<p><strong>Reward Hypothesis:</strong> All goals can be described by the maximisation of expected cumulative reward</p>\n</blockquote>\n<p>在强化学习中，通常会有一个预设的目标，如在试图在围棋对弈中取胜，围棋中的每一步行子并不会有绝对的正误之分，但却会对最终能否取胜产生影响，我们可以分析每一步对于最终获胜的贡献程度而选择最佳的落子方式。</p>\n<p>与其他的学习方式相比，RL具有如下特征：</p>\n<ol>\n<li>不存在监督者，只有奖励信号</li>\n<li>反馈会延迟，而非即时对应，一个动作的反馈信号可能会很久后到来</li>\n<li>时间很重要，RL是一个动态的过程，这一过程经常被马尔科夫决策过程所描述，动作-状态对是序列发生的，并不能通过i.i.d数据进行学习；结合第二点，动作的反馈可能有长时与短时之分，短时获益的选择可能会导致一段时间后的损失，眼下并不能获益的行为可能最有助于达到最终的目标。</li>\n<li>如上所述，学习器所获得的信号并非是固定的，而是由其自身的行为决定。某一时刻下学习器的选择会决定之后的状态。</li>\n</ol>\n<p>我们所能够遇到的场景通常也都与RL所假设的一致，总体上看是在探索未知领域与遵从已有知识间找到平衡。这种目标导向的学习方式相较于监督学习与非监督学习也更加灵活，也被认为是人类最为主要的学习方式之一。</p>\n<h2 id=\"3-2-主动学习器与被动学习器\"><a href=\"#3-2-主动学习器与被动学习器\" class=\"headerlink\" title=\"3.2 主动学习器与被动学习器\"></a>3.2 主动学习器与被动学习器</h2><p>学习器可以根据与获取训练数据的方式分为主动学习器和被动学习器。</p>\n<p>主动学习器会在学习时通过提问或实验的方式与环境交互，如一个学习器在学习标记垃圾邮件时，会主动挑选出一些邮件要求用户标记，从而提高对“垃圾邮件的理解”。</p>\n<p>被动学习器则完全依据事先提供的信息进行学习。</p>\n<h2 id=\"3-3-在线学习与批量学习\"><a href=\"#3-3-在线学习与批量学习\" class=\"headerlink\" title=\"3.3 在线学习与批量学习\"></a>3.3 在线学习与批量学习</h2><p>批量学习指一次性处理大量的数据后获取技能；在线学习指在新数据到来后，立即考虑新数据的影响并对学习结果进行更新，一般而言，这样的学习结果更加有效。而从人类的学习与分析表现上看，应当是具有实现类似于在线学习的能力。</p>\n<h1 id=\"4-学习的方法\"><a href=\"#4-学习的方法\" class=\"headerlink\" title=\"4. 学习的方法\"></a>4. 学习的方法</h1><p>关于学习理论，有许多不同的流派持有不同的看法。这里就产生式分类和判别式分类，频率派与贝叶斯派的基本方法进行简单的介绍。</p>\n<h2 id=\"4-1-生成的分类与识别的分类\"><a href=\"#4-1-生成的分类与识别的分类\" class=\"headerlink\" title=\"4.1 生成的分类与识别的分类\"></a>4.1 生成的分类与识别的分类</h2><p>考虑如果已知观测对象是模式$\\boldsymbol{x}$，如这是急性昏迷病人的脑电模式，使得分类类别$y$（预后：转醒，PVS）在模式$\\boldsymbol{x}$下的条件概率$p(y\\mid\\boldsymbol{x})$最大的的类别$\\hat{y}$，就是这一病人相匹配的预后类别。<br>$$\\hat{y}=\\mathrm{argmax; }p_y(y\\mid\\boldsymbol{x})$$<br>这里”$\\mathrm{argmax}$ “是取最大值时的参数的意思。在模式识别里条件概率$p(y\\mid\\boldsymbol{x})$常被称作后验概率。应用训练集直接对后验概率$p(y\\mid\\boldsymbol{x})$进行学习的过程，称为判别式分析。</p>\n<p>此外，还可以把后验概率$p(y\\mid\\boldsymbol{x})$表示为$y$的函数<br>$$<br>p(y\\mid\\boldsymbol{x})=\\frac{p(\\boldsymbol{x},y)}{p(\\boldsymbol{x})}\\propto{p(\\boldsymbol{x},y)}<br>$$<br>根据上式，我们可以发现模式$\\boldsymbol{x}$和类别$y$的联合概率$p(\\boldsymbol{x},y)$与后验概率$p(y|\\boldsymbol{x})$是成正比的。因此，我们可以通过使联合概率$p(\\boldsymbol{x},y)$达到最大值的方法，得到使后验概率$p(y\\mid\\boldsymbol{x})$达到最大值的类别$\\hat{y}$。<br>$$<br>\\hat{y}=\\mathrm{argmax;}p_y(\\boldsymbol{x},y)<br>$$<br>在模式识别力，联合概率$p(\\boldsymbol{x},y)$也被成为数据生成概率，通过预测数据生成概率$p(\\boldsymbol{x},y)$来进行模式识别的分类方法，称为生成的分类。</p>\n<p>一般而言，解决一个特殊问题所需的信息小于解决这个特殊问题所属的一般问题的信息。因为生成概率与后验概率的关系为：<br>$$<br>p(y\\mid\\boldsymbol{x})=\\frac{p(\\boldsymbol{x},y)}{p(\\boldsymbol{x})}=\\frac{p(\\boldsymbol{x},y)}{\\Sigma_yp(\\boldsymbol{x},y)}<br>$$<br>在已知生成概率$p(\\boldsymbol{x},y)$的情况下可以很容易地得到后验概率$p(y\\mid\\boldsymbol{x})$，而已知后验概率$p(y\\mid\\boldsymbol{x})$的情况下却难以得到生成概率$p(\\boldsymbol{x},y)$。</p>\n<p>在具有一定先验知识的情况下，可以事先得到生成概率$p(\\boldsymbol{x},y)$，在先验概率的约束下，相当于具有了归纳偏置的能力，能对推断结果有更好的限制，相对而言生成的分类是更好的方式；而在没有先验知识，不知道生成概率$p(\\boldsymbol{x},y)$的情况下，则仅能够通过后验概率$p(y\\mid\\boldsymbol{x})$使用识别的分类进行推断了。</p>\n<h2 id=\"4-2-统计概率和朴素贝叶斯\"><a href=\"#4-2-统计概率和朴素贝叶斯\" class=\"headerlink\" title=\"4.2 统计概率和朴素贝叶斯\"></a>4.2 统计概率和朴素贝叶斯</h2><p>统计概率的学习方法将对某一模式的描述看作是学习的结果；学习的目的是通过训练集如$\\mathcal{D}$，得到对包含参数$\\boldsymbol{\\theta}$的模式$q(\\boldsymbol{x},y;\\boldsymbol{\\theta})$的精确描述。<br>$$<br>\\max_\\theta\\prod_{i=1}^{y}q(\\boldsymbol{x}_i,y_i;\\boldsymbol{\\theta})<br>$$<br>朴素贝叶斯方法则将从模式$\\boldsymbol{\\theta}$产生的角度出发，将其看作是概率变量；依据贝叶斯定理，就可以通过求得的后验概率$p(\\boldsymbol{\\theta}\\mid\\mathcal{D})$估计先验概率$p(\\boldsymbol{\\theta})$。<br>$$<br>p(\\boldsymbol{\\theta}\\mid\\mathcal{D})=\\frac{p(\\mathcal{D}\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathcal{D})}=\\frac{\\prod_{i=1}^nq(\\boldsymbol{x}_i,y_i\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{\\int\\prod_{i=1}^nq(\\boldsymbol{x}_i,y_i\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\mathrm{d}\\boldsymbol{\\theta}}<br>$$</p>\n<h1 id=\"5-学习的理论框架\"><a href=\"#5-学习的理论框架\" class=\"headerlink\" title=\"5. 学习的理论框架\"></a>5. 学习的理论框架</h1><h2 id=\"5-1-学习器的输入\"><a href=\"#5-1-学习器的输入\" class=\"headerlink\" title=\"5.1 学习器的输入\"></a>5.1 学习器的输入</h2><h3 id=\"领域集-domain-set\"><a href=\"#领域集-domain-set\" class=\"headerlink\" title=\"领域集(domain set)\"></a>领域集(domain set)</h3><p>领域集即我们希望学习的对象的总体，可以是任意的集合$\\mathcal{X}$。领域集中的元素通常可以用描述了其特征的向量表示，从心理学研究问题的角度出发，这些特征就是有关研究对象的因素，每个因素又存在不同的水平。如对于木瓜而言，其具有的特征有颜色，软硬程度等，对于颜色特征而言又有绿色，黄色等水平。在围棋对弈中，可以落子的位置构成了当前状态下的领域集。</p>\n<p>领域集中的元素也叫做实例，因此$\\mathcal{X}$也被叫做实例空间。</p>\n<h3 id=\"标签集-label-set\"><a href=\"#标签集-label-set\" class=\"headerlink\" title=\"标签集(label set)\"></a>标签集(label set)</h3><p>标签是指我们希望通过学习得到的有关实例的信息，标签集$\\mathcal{Y}$即是所有标签的集合。就木瓜的例子而言，我们会希望通过对于木瓜特征的学习，辨别木瓜好不好吃，因此给予木瓜的标签可以是好吃（为方便起见，用$1$表示）和不好吃（用$0$表示），因此对于木瓜的领域集$\\mathcal{X}$而言，其标签集$\\mathcal{Y}$为${1,0}$。</p>\n<h3 id=\"训练数据-training-data\"><a href=\"#训练数据-training-data\" class=\"headerlink\" title=\"训练数据(training data)\"></a>训练数据(training data)</h3><p>为了使得学习器学习到有关领域集的知识，通常我们都需要获得该领域集中的一些样本供学习器学习。训练数据至少应包含领域集内实例的部分特征，在监督学习情况下，训练数据还应包含实例所对应的标签，即训练数据以$\\mathcal{X}\\times\\mathcal{Y}$的形式成对出现，训练数据可以表示为训练集$S={(X_1,Y_1),\\cdots,(x_m,y_m)}$这样一个有限的序列。</p>\n<h2 id=\"5-2-学习器的输出\"><a href=\"#5-2-学习器的输出\" class=\"headerlink\" title=\"5.2 学习器的输出\"></a>5.2 学习器的输出</h2><p>学习器学习的目的是从训练数据中学会适用于训练数据所属领域集的知识，即可以给未加标签的实例加上预测的标签，这样的过程可以用一个描述从$\\mathcal{X}$到$\\mathcal{Y}$映射的函数表示，$h:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，这样的映射被称为预测规则(prediction rule)，这样的函数也可以被称为预测器(predictor)，假设(hypothesis)或分类器(classifier)。</p>\n<p>通常我们用$A(S)$来表示学习算法$A$在给定的训练序列$S$下返回的假设。</p>\n<h2 id=\"5-3-获取训练数据\"><a href=\"#5-3-获取训练数据\" class=\"headerlink\" title=\"5.3 获取训练数据\"></a>5.3 获取训练数据</h2><p>从领域集中抽取合适的样本实际上是个很复杂的问题，通常我们会希望抽取的样本能与领域集独立同分布(i.i.d)，即训练数据$\\mathcal{S}$亦从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中抽取，服从$\\mathcal{D}$的分布。</p>\n<p>我们假设，对领域集而言，实例特征与标签间存在正确的映射关系$f:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，对于任意的$i$均有$y_i=f(x_i)$。学习器通过算法$A$学习训练数据，以正确的映射关系$f(x)$为目标，寻找$\\mathcal{X}$与$\\mathcal{Y}$间的映射关系，为训练数据赋予正确的标签，最终产生学习到的映射关系$h(x)$。</p>\n<h2 id=\"5-4-评估学习结果\"><a href=\"#5-4-评估学习结果\" class=\"headerlink\" title=\"5.4 评估学习结果\"></a>5.4 评估学习结果</h2><p>分类器的误差被定义为，从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中随机抽取一个样本$x$，使得$h(x)\\boldsymbol\\ne f(x)$的概率。</p>\n<p>$h:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$的错误率被定义为$\\newcommand{\\eqdef}{\\overset{\\mathrm{def}}{=}}$ </p>\n<p>$$<br>L_{\\mathcal{D},f}\\eqdef\\mathbb{P}_{x\\sim\\mathcal{D}}[h(x)\\ne f(x)]\\eqdef\\mathcal{D}({x:h(x)\\neq f(x)})<br>$$<br>$L_{\\mathcal{D},f}(h)$也称为泛化误差，真实误差，或损失函数(cost function)，其下标$(\\mathcal{D},f)$表明误差的测量是基于概率分布$\\mathcal{D}$和正确标记函数$f(x)$进行的。在接下来对于PAC可学习性的讨论中，还会讨论这种损失的其他的可能形式。</p>\n<h1 id=\"6-经验风险最小化\"><a href=\"#6-经验风险最小化\" class=\"headerlink\" title=\"6. 经验风险最小化\"></a>6. 经验风险最小化</h1><h2 id=\"6-1-经验风险\"><a href=\"#6-1-经验风险\" class=\"headerlink\" title=\"6.1 经验风险\"></a>6.1 经验风险</h2><p>学习的过程是从领域集$\\mathcal{X}$的分布$\\mathcal{D}$中获取训练集$S$，我们希望学习器能通过某种算法得到一个预测器$h_S:\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$，使得关于分布$\\mathcal{D}$和正确标记函数$f(x)$的预测误差最小化。</p>\n<p>由于我们并不知道分布$\\mathcal{D}$和正确标记函数$f(x)$究竟是什么样的，因此我们需要借助学习器在训练样本中的误差来对犯错的风险进行估计：<br>$$<br>L_S(h)\\eqdef\\frac{\\vert{i\\in[m]:h(x_i)\\ne y_i}\\vert}{m}<br>$$<br>其中，$[m]={1,\\cdots,m}$</p>\n<p>这种从训练数据中得到的对风险的估计被称为经验风险或经验误差。由于我们假设训练集能够代表领域集的特征，因此，这种从训练集估计$\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$映射并以经验风险代表估计的真实风险的做法是可以接受的。这种寻求拥有最小经验风险的过程称为经验风险最小化(Empirical Risk Minization，ERM)。</p>\n<h2 id=\"6-2-过拟合\"><a href=\"#6-2-过拟合\" class=\"headerlink\" title=\"6.2 过拟合\"></a>6.2 过拟合</h2><p>由于训练数据具有有限的数据量，如果不考虑寻找到的规则$h(x)$的复杂度，一定能够找到一种方法，将训练集中$\\mathcal{X}\\boldsymbol{\\to}\\mathcal{Y}$的关系拟合得很好。一个极端的例子是，如果学习过程变成了单纯的记忆，那么$L_\\mathcal{D}(h_S)$一定会等于$0$。</p>\n<p>通常过拟合发生时会导致，对训练数据拟合得非常好，但在训练数据之外表现得非常糟糕。</p>\n<h1 id=\"7-考虑归纳偏置的经验风险最小化\"><a href=\"#7-考虑归纳偏置的经验风险最小化\" class=\"headerlink\" title=\"7. 考虑归纳偏置的经验风险最小化\"></a>7. 考虑归纳偏置的经验风险最小化</h1><h2 id=\"7-1-归纳偏置的影响\"><a href=\"#7-1-归纳偏置的影响\" class=\"headerlink\" title=\"7.1 归纳偏置的影响\"></a>7.1 归纳偏置的影响</h2><p>过拟合产生的一个重要原因在于，学习器对于真实的映射关系$f(x)$没有任何先验的概念，即不知道$f(x)$应该有的样子，$f(x)$本身应该有的性质无法对本应与$f(x)$一致的$h(x)$产生限制，$h(x)$为了适应数据而可以不考虑复杂度变得非常奇怪。</p>\n<p>又或者说，如果仅仅知道晚饭吃饱了而去推测晚上吃了什么，那么无论是中餐、西餐、日料、压缩饼干甚至是某些奇奇怪怪的东西都可以满足这一要求，但如果事先知道这是个中国人，而且是南方人，那么猜测的范围就可以大大缩小了。</p>\n<p>因此，在学习器接触训练集开始学习之前，就可以通过基于所学学习问题的先验知识，将基于训练集$S$的可能的选择器$h(x)$的范围限制于一个集合$\\mathcal{H}$中，这样的一个集合被称为假设类。在ERM准则下，假设类利用ERM规则进一步选择出合适的预测器$h\\boldsymbol\\in\\mathcal{H}$，这一过程可用如下形式表示<br>$$<br>ERM_\\mathcal{H}(S)\\in\\mathrm{argmax;}_{h\\in\\mathcal{H}}L_S(h)<br>$$<br>接下来将会证明，为什么基于此假设类的$\\mathrm{ERM}_\\mathcal{H}$能够保证不过拟合，以及选择哪种假设类$\\mathrm{ERM}_\\mathcal{H}$不会导致过拟合。</p>\n<h2 id=\"7-2-真实风险的来源\"><a href=\"#7-2-真实风险的来源\" class=\"headerlink\" title=\"7.2 真实风险的来源\"></a>7.2 真实风险的来源</h2><h3 id=\"置信参数-confidence-parameter\"><a href=\"#置信参数-confidence-parameter\" class=\"headerlink\" title=\"置信参数(confidence parameter)\"></a>置信参数(confidence parameter)</h3><p>由于学习器仅能通过训练集$S$学习在$\\mathcal{D}$上分布的领域集$\\mathcal{X}$，因此$S$与$\\mathcal{X}$分布的差异会导致真实误差$L_{\\mathcal{H},f}$的增加。为保证学习器所学习到的$h(x)$的准确性，需要保证$S$中的训练样本满足独立同分布(i.i.d.)假设，都是从$\\mathcal{D}$中独立同分布地抽取的，记为$S\\boldsymbol\\sim\\mathcal{D}^m$，其中$m$为$S$的势，即为在分布$\\mathcal{D}$下，独立同分布采样$S\\boldsymbol=z_1,\\cdots,z_m$。</p>\n<p>然而实际上，我们难以保证抽样的独立同分布，即使一学期仅有三次点名，一学期也仅skip三次课，也是有可能全部赶上点名的: )</p>\n<p>因此，我们将采样到不能代表领域集$\\mathcal{X}$的训练集$S$的概率表示为$\\delta$，并将$1-\\delta$称为该预测的置信参数(confidence parameter)</p>\n<h3 id=\"精度参数-accuracy-parameter\"><a href=\"#精度参数-accuracy-parameter\" class=\"headerlink\" title=\"精度参数(accuracy parameter)\"></a>精度参数(accuracy parameter)</h3><p>同样的，我们也无法保证$h(x)$所给出的预测结果的正确性，此时，我们使用精度参数(accuracy parameter)$\\varepsilon$来评价$h(x)$预测的质量，如果$L_{\\mathcal{D},f}&gt;\\varepsilon$，那么$h(x)$就是一种失败的预测；如果$L_{\\mathcal{D},f}{\\leq}\\varepsilon$，那么就认为$h(x)$给出了一个误差可以接受的预测。</p>\n<h2 id=\"7-3-学习的可靠性\"><a href=\"#7-3-学习的可靠性\" class=\"headerlink\" title=\"7.3 学习的可靠性\"></a>7.3 学习的可靠性</h2><p>对于$S\\sim\\mathcal{D}^m$的训练集来说，设$\\mathcal{H}_B$是失败的假设集合，那么有：<br>$$<br>\\mathcal{H}_B={h\\in\\mathcal{H}:L_{\\mathcal{D},f}(h)&gt;\\varepsilon}<br>$$<br>设集合$M$是一个误导集，<br>$$<br>M={S\\mid_x \\exists :h\\in\\mathcal{H}_B,L_S(h)=0}<br>$$</p>\n<p>即$M$中的假设对于领域集来说是失败的预测，但对于训练集来说，则是符合精度参数要求的。即$M$描述了过拟合的$h(x)$。</p>"}],"PostAsset":[],"PostCategory":[{"post_id":"ckl5145an0005ysm6hswz8t7e","category_id":"ckl5145ar000aysm6cjcmalz7","_id":"ckl5145au000gysm6h8yk4kde"}],"PostTag":[{"post_id":"ckl5145an0005ysm6hswz8t7e","tag_id":"ckl5145at000eysm67eose1d1","_id":"ckl5145au000hysm602ekbd9l"}],"Tag":[{"name":"tmux","_id":"ckl5145al0003ysm6goj46h4v"},{"name":"npm","_id":"ckl5145aq0007ysm6a9gu1pos"},{"name":"life","_id":"ckl5145as000bysm68y5u1qim"},{"name":"Learning Theory","_id":"ckl5145at000eysm67eose1d1"}]}}